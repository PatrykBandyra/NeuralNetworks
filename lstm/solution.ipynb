{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled3.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n"
   ],
   "metadata": {
    "id": "YgioygiQti60"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h7YQkK0MZ--y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Seeds"
   ],
   "metadata": {
    "id": "HTnQFZ7ifkd-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)  # Sets up seed for both devices\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {
    "id": "zoKW7kR5fqT5"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "id": "qdz5FgGwfyi4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yC8b-eHCeuvU",
    "outputId": "629b1d32-f97a-4c71-a004-ee5f68eb2a52"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Properties"
   ],
   "metadata": {
    "id": "-Pe7Cx3NHk5U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "vocab_size = 200\n",
    "embedding_dim = 10\n",
    "\n",
    "hidden_size = 128\n",
    "n_layers = 3\n",
    "out_size = 5\n",
    "proj_size = 3"
   ],
   "metadata": {
    "id": "Hb459ljAHtYG"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "id": "Nuz1S4Katekd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_file = open('train.pkl', 'rb')\n",
    "dataset = pickle.load(train_file)\n",
    "dataset = [(i[0]+1, i[1]) for i in dataset]\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[round(len(dataset)*0.3):]\n",
    "test_dataset = dataset[:round(len(dataset)*0.3)]"
   ],
   "metadata": {
    "id": "aBpHzH4aexO6"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = Tensor([len(x) for x in xx])\n",
    "\n",
    "  xx = [Tensor(i) for i in xx]\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy = Tensor(yy)\n",
    "\n",
    "  return xx_pad, x_lens, yy"
   ],
   "metadata": {
    "id": "sTwWHuS3G1T6"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
   ],
   "metadata": {
    "id": "ZUBPdHDgGv5y"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n"
   ],
   "metadata": {
    "id": "QUStZIj4Id8E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LSTM_Seq_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #params\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        #embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #lstm\n",
    "        self.lstm = lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, num_layers = n_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        #dense layer\n",
    "        self.dense1 = nn.Linear(2*hidden_size, out_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(2*self.num_layers, batch_size, self.hidden_size)\n",
    "        state = torch.zeros(2*self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "    \n",
    "    def forward(self, x, x_lens, hidden):\n",
    "        embeddings = self.embedding(x)\n",
    "        packed_embeddings = pack_padded_sequence(embeddings, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.lstm(packed_embeddings, hidden)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out_forward = out[range(len(out)), x_lens - 1, :self.hidden_size]\n",
    "        out_reverse = out[:, 0, self.hidden_size:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        lin1 = self.act(self.dense1(out_reduced))\n",
    "        # soft = self.soft(lin1)\n",
    "        return lin1, hidden"
   ],
   "metadata": {
    "id": "SxeMI5MuIfhA"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "id": "9ArYeTH6MTI7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = LSTM_Seq_Classifier(embedding_dim, hidden_size, n_layers, out_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "\n",
    "  for i, (x_padded, x_lens, y) in enumerate(data_loader):\n",
    "    x_padded, x_lens, y = x_padded.to(torch.int64), x_lens.to(torch.int64), y.to(torch.int64)\n",
    "    x_padded, y = x_padded.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    hidden, state = model.init_hidden(x_padded.size(1))\n",
    "    hidden, state = hidden.to(device), state.to(device)\n",
    "    \n",
    "    out, last_hidden = model(x_padded, x_lens, (hidden, state))\n",
    "    \n",
    "\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%20 == 0:\n",
    "      model.eval()\n",
    "\n",
    "      test_loss = 0.0\n",
    "      test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "      for j, (x_padded, x_lens, y) in enumerate(test_loader):\n",
    "        x_padded, x_lens, y = x_padded.to(torch.int64), x_lens.to(torch.int64), y.to(torch.int64)\n",
    "        x_padded, y = x_padded.to(device), y.to(device)\n",
    "        hidden, state = model.init_hidden(x_padded.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        out, last_hidden = model(x_padded, x_lens, (hidden,state))\n",
    "        test_loss = test_loss + criterion(out, y)\n",
    "      \n",
    "      print(f'Epoch: {epoch}, i: {i}, train_loss: {loss.item():.3}, test_loss: {test_loss.item()/j+1:.3}')\n",
    "\n",
    "      model.train()"
   ],
   "metadata": {
    "id": "g5ElBuEaMXE2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "a2138107-76c0-4ff9-dceb-15b93952ae32"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, i: 0, train_loss: 1.6, test_loss: 2.64\n",
      "Epoch: 0, i: 20, train_loss: 1.29, test_loss: 2.36\n",
      "Epoch: 0, i: 40, train_loss: 1.02, test_loss: 2.22\n",
      "Epoch: 0, i: 60, train_loss: 1.41, test_loss: 2.33\n",
      "Epoch: 1, i: 0, train_loss: 1.15, test_loss: 2.25\n",
      "Epoch: 1, i: 20, train_loss: 0.998, test_loss: 2.2\n",
      "Epoch: 1, i: 40, train_loss: 1.03, test_loss: 2.19\n",
      "Epoch: 1, i: 60, train_loss: 1.08, test_loss: 2.21\n",
      "Epoch: 2, i: 0, train_loss: 1.19, test_loss: 2.19\n",
      "Epoch: 2, i: 20, train_loss: 0.899, test_loss: 2.12\n",
      "Epoch: 2, i: 40, train_loss: 0.942, test_loss: 2.11\n",
      "Epoch: 2, i: 60, train_loss: 1.21, test_loss: 2.21\n",
      "Epoch: 3, i: 0, train_loss: 0.934, test_loss: 2.12\n",
      "Epoch: 3, i: 20, train_loss: 0.855, test_loss: 2.05\n",
      "Epoch: 3, i: 40, train_loss: 0.868, test_loss: 2.09\n",
      "Epoch: 3, i: 60, train_loss: 1.13, test_loss: 2.29\n",
      "Epoch: 4, i: 0, train_loss: 1.06, test_loss: 2.28\n",
      "Epoch: 4, i: 20, train_loss: 1.36, test_loss: 2.23\n",
      "Epoch: 4, i: 40, train_loss: 1.23, test_loss: 2.19\n",
      "Epoch: 4, i: 60, train_loss: 1.14, test_loss: 2.22\n",
      "Epoch: 5, i: 0, train_loss: 1.06, test_loss: 2.19\n",
      "Epoch: 5, i: 20, train_loss: 0.901, test_loss: 2.15\n",
      "Epoch: 5, i: 40, train_loss: 1.13, test_loss: 2.09\n",
      "Epoch: 5, i: 60, train_loss: 0.946, test_loss: 2.08\n",
      "Epoch: 6, i: 0, train_loss: 1.09, test_loss: 2.08\n",
      "Epoch: 6, i: 20, train_loss: 0.972, test_loss: 2.06\n",
      "Epoch: 6, i: 40, train_loss: 1.08, test_loss: 2.07\n",
      "Epoch: 6, i: 60, train_loss: 0.974, test_loss: 2.03\n",
      "Epoch: 7, i: 0, train_loss: 0.909, test_loss: 2.15\n",
      "Epoch: 7, i: 20, train_loss: 0.929, test_loss: 2.1\n",
      "Epoch: 7, i: 40, train_loss: 1.01, test_loss: 2.06\n",
      "Epoch: 7, i: 60, train_loss: 0.992, test_loss: 2.1\n",
      "Epoch: 8, i: 0, train_loss: 1.2, test_loss: 2.06\n",
      "Epoch: 8, i: 20, train_loss: 0.764, test_loss: 2.02\n",
      "Epoch: 8, i: 40, train_loss: 1.26, test_loss: 2.11\n",
      "Epoch: 8, i: 60, train_loss: 0.795, test_loss: 2.02\n",
      "Epoch: 9, i: 0, train_loss: 1.23, test_loss: 2.1\n",
      "Epoch: 9, i: 20, train_loss: 0.948, test_loss: 2.15\n",
      "Epoch: 9, i: 40, train_loss: 0.777, test_loss: 1.94\n",
      "Epoch: 9, i: 60, train_loss: 0.806, test_loss: 2.08\n",
      "Epoch: 10, i: 0, train_loss: 0.894, test_loss: 1.93\n",
      "Epoch: 10, i: 20, train_loss: 0.717, test_loss: 1.91\n",
      "Epoch: 10, i: 40, train_loss: 0.573, test_loss: 1.99\n",
      "Epoch: 10, i: 60, train_loss: 1.15, test_loss: 1.9\n",
      "Epoch: 11, i: 0, train_loss: 1.04, test_loss: 2.05\n",
      "Epoch: 11, i: 20, train_loss: 0.678, test_loss: 1.84\n",
      "Epoch: 11, i: 40, train_loss: 0.54, test_loss: 1.85\n",
      "Epoch: 11, i: 60, train_loss: 0.962, test_loss: 1.99\n",
      "Epoch: 12, i: 0, train_loss: 1.11, test_loss: 2.04\n",
      "Epoch: 12, i: 20, train_loss: 0.772, test_loss: 1.85\n",
      "Epoch: 12, i: 40, train_loss: 0.721, test_loss: 1.8\n",
      "Epoch: 12, i: 60, train_loss: 0.725, test_loss: 1.82\n",
      "Epoch: 13, i: 0, train_loss: 0.453, test_loss: 1.87\n",
      "Epoch: 13, i: 20, train_loss: 0.611, test_loss: 1.97\n",
      "Epoch: 13, i: 40, train_loss: 0.939, test_loss: 1.78\n",
      "Epoch: 13, i: 60, train_loss: 0.78, test_loss: 1.77\n",
      "Epoch: 14, i: 0, train_loss: 0.483, test_loss: 1.77\n",
      "Epoch: 14, i: 20, train_loss: 0.425, test_loss: 1.79\n",
      "Epoch: 14, i: 40, train_loss: 0.634, test_loss: 1.74\n",
      "Epoch: 14, i: 60, train_loss: 0.809, test_loss: 1.73\n",
      "Epoch: 15, i: 0, train_loss: 0.447, test_loss: 1.74\n",
      "Epoch: 15, i: 20, train_loss: 0.519, test_loss: 1.73\n",
      "Epoch: 15, i: 40, train_loss: 0.639, test_loss: 1.72\n",
      "Epoch: 15, i: 60, train_loss: 0.577, test_loss: 1.7\n",
      "Epoch: 16, i: 0, train_loss: 0.456, test_loss: 1.69\n",
      "Epoch: 16, i: 20, train_loss: 0.41, test_loss: 1.66\n",
      "Epoch: 16, i: 40, train_loss: 0.287, test_loss: 1.68\n",
      "Epoch: 16, i: 60, train_loss: 0.61, test_loss: 1.67\n",
      "Epoch: 17, i: 0, train_loss: 0.532, test_loss: 1.66\n",
      "Epoch: 17, i: 20, train_loss: 0.604, test_loss: 1.65\n",
      "Epoch: 17, i: 40, train_loss: 0.328, test_loss: 1.66\n",
      "Epoch: 17, i: 60, train_loss: 0.39, test_loss: 1.73\n",
      "Epoch: 18, i: 0, train_loss: 0.286, test_loss: 1.69\n",
      "Epoch: 18, i: 20, train_loss: 0.453, test_loss: 1.67\n",
      "Epoch: 18, i: 40, train_loss: 0.494, test_loss: 1.68\n",
      "Epoch: 18, i: 60, train_loss: 0.493, test_loss: 1.67\n",
      "Epoch: 19, i: 0, train_loss: 0.768, test_loss: 1.67\n",
      "Epoch: 19, i: 20, train_loss: 0.137, test_loss: 1.66\n",
      "Epoch: 19, i: 40, train_loss: 0.369, test_loss: 1.66\n",
      "Epoch: 19, i: 60, train_loss: 0.486, test_loss: 1.68\n",
      "Epoch: 20, i: 0, train_loss: 0.383, test_loss: 1.66\n",
      "Epoch: 20, i: 20, train_loss: 0.808, test_loss: 1.67\n",
      "Epoch: 20, i: 40, train_loss: 0.561, test_loss: 1.67\n",
      "Epoch: 20, i: 60, train_loss: 0.175, test_loss: 1.63\n",
      "Epoch: 21, i: 0, train_loss: 0.489, test_loss: 1.67\n",
      "Epoch: 21, i: 20, train_loss: 0.286, test_loss: 1.64\n",
      "Epoch: 21, i: 40, train_loss: 0.485, test_loss: 1.64\n",
      "Epoch: 21, i: 60, train_loss: 0.415, test_loss: 1.63\n",
      "Epoch: 22, i: 0, train_loss: 0.489, test_loss: 1.63\n",
      "Epoch: 22, i: 20, train_loss: 0.73, test_loss: 1.68\n",
      "Epoch: 22, i: 40, train_loss: 0.439, test_loss: 1.64\n",
      "Epoch: 22, i: 60, train_loss: 0.378, test_loss: 1.64\n",
      "Epoch: 23, i: 0, train_loss: 0.188, test_loss: 1.64\n",
      "Epoch: 23, i: 20, train_loss: 0.254, test_loss: 1.68\n",
      "Epoch: 23, i: 40, train_loss: 0.707, test_loss: 1.74\n",
      "Epoch: 23, i: 60, train_loss: 0.488, test_loss: 1.62\n",
      "Epoch: 24, i: 0, train_loss: 0.505, test_loss: 1.67\n",
      "Epoch: 24, i: 20, train_loss: 0.311, test_loss: 1.68\n",
      "Epoch: 24, i: 40, train_loss: 0.28, test_loss: 1.65\n",
      "Epoch: 24, i: 60, train_loss: 0.187, test_loss: 1.64\n",
      "Epoch: 25, i: 0, train_loss: 0.428, test_loss: 1.66\n",
      "Epoch: 25, i: 20, train_loss: 0.751, test_loss: 1.67\n",
      "Epoch: 25, i: 40, train_loss: 0.152, test_loss: 1.66\n",
      "Epoch: 25, i: 60, train_loss: 0.244, test_loss: 1.65\n",
      "Epoch: 26, i: 0, train_loss: 0.179, test_loss: 1.69\n",
      "Epoch: 26, i: 20, train_loss: 0.336, test_loss: 1.63\n",
      "Epoch: 26, i: 40, train_loss: 0.429, test_loss: 1.65\n",
      "Epoch: 26, i: 60, train_loss: 0.296, test_loss: 1.63\n",
      "Epoch: 27, i: 0, train_loss: 0.268, test_loss: 1.64\n",
      "Epoch: 27, i: 20, train_loss: 0.139, test_loss: 1.61\n",
      "Epoch: 27, i: 40, train_loss: 0.249, test_loss: 1.63\n",
      "Epoch: 27, i: 60, train_loss: 0.506, test_loss: 1.67\n",
      "Epoch: 28, i: 0, train_loss: 0.262, test_loss: 1.67\n",
      "Epoch: 28, i: 20, train_loss: 0.314, test_loss: 1.61\n",
      "Epoch: 28, i: 40, train_loss: 0.253, test_loss: 1.64\n",
      "Epoch: 28, i: 60, train_loss: 0.241, test_loss: 1.66\n",
      "Epoch: 29, i: 0, train_loss: 0.212, test_loss: 1.64\n",
      "Epoch: 29, i: 20, train_loss: 0.0968, test_loss: 1.62\n",
      "Epoch: 29, i: 40, train_loss: 0.337, test_loss: 1.65\n",
      "Epoch: 29, i: 60, train_loss: 0.131, test_loss: 1.66\n",
      "Epoch: 30, i: 0, train_loss: 0.234, test_loss: 1.66\n",
      "Epoch: 30, i: 20, train_loss: 0.207, test_loss: 1.7\n",
      "Epoch: 30, i: 40, train_loss: 0.118, test_loss: 1.64\n",
      "Epoch: 30, i: 60, train_loss: 0.29, test_loss: 1.69\n",
      "Epoch: 31, i: 0, train_loss: 0.324, test_loss: 1.8\n",
      "Epoch: 31, i: 20, train_loss: 0.236, test_loss: 1.72\n",
      "Epoch: 31, i: 40, train_loss: 0.31, test_loss: 1.75\n",
      "Epoch: 31, i: 60, train_loss: 0.205, test_loss: 1.66\n",
      "Epoch: 32, i: 0, train_loss: 0.219, test_loss: 1.65\n",
      "Epoch: 32, i: 20, train_loss: 0.281, test_loss: 1.7\n",
      "Epoch: 32, i: 40, train_loss: 0.0886, test_loss: 1.66\n",
      "Epoch: 32, i: 60, train_loss: 0.275, test_loss: 1.7\n",
      "Epoch: 33, i: 0, train_loss: 0.133, test_loss: 1.64\n",
      "Epoch: 33, i: 20, train_loss: 0.119, test_loss: 1.67\n",
      "Epoch: 33, i: 40, train_loss: 0.0201, test_loss: 1.68\n",
      "Epoch: 33, i: 60, train_loss: 0.446, test_loss: 1.66\n",
      "Epoch: 34, i: 0, train_loss: 0.0863, test_loss: 1.67\n",
      "Epoch: 34, i: 20, train_loss: 0.208, test_loss: 1.66\n",
      "Epoch: 34, i: 40, train_loss: 0.101, test_loss: 1.66\n",
      "Epoch: 34, i: 60, train_loss: 0.0973, test_loss: 1.64\n",
      "Epoch: 35, i: 0, train_loss: 0.183, test_loss: 1.65\n",
      "Epoch: 35, i: 20, train_loss: 0.113, test_loss: 1.66\n",
      "Epoch: 35, i: 40, train_loss: 0.0873, test_loss: 1.66\n",
      "Epoch: 35, i: 60, train_loss: 0.196, test_loss: 1.69\n",
      "Epoch: 36, i: 0, train_loss: 0.219, test_loss: 1.69\n",
      "Epoch: 36, i: 20, train_loss: 0.234, test_loss: 1.67\n",
      "Epoch: 36, i: 40, train_loss: 0.0711, test_loss: 1.68\n",
      "Epoch: 36, i: 60, train_loss: 0.262, test_loss: 1.69\n",
      "Epoch: 37, i: 0, train_loss: 0.108, test_loss: 1.72\n",
      "Epoch: 37, i: 20, train_loss: 0.0674, test_loss: 1.71\n",
      "Epoch: 37, i: 40, train_loss: 0.21, test_loss: 1.79\n",
      "Epoch: 37, i: 60, train_loss: 0.43, test_loss: 1.73\n",
      "Epoch: 38, i: 0, train_loss: 0.272, test_loss: 1.75\n",
      "Epoch: 38, i: 20, train_loss: 0.121, test_loss: 1.72\n",
      "Epoch: 38, i: 40, train_loss: 0.216, test_loss: 1.72\n",
      "Epoch: 38, i: 60, train_loss: 0.156, test_loss: 1.71\n",
      "Epoch: 39, i: 0, train_loss: 0.116, test_loss: 1.68\n",
      "Epoch: 39, i: 20, train_loss: 0.277, test_loss: 1.66\n",
      "Epoch: 39, i: 40, train_loss: 0.208, test_loss: 1.64\n",
      "Epoch: 39, i: 60, train_loss: 0.0837, test_loss: 1.8\n",
      "Epoch: 40, i: 0, train_loss: 0.113, test_loss: 1.76\n",
      "Epoch: 40, i: 20, train_loss: 0.024, test_loss: 1.7\n",
      "Epoch: 40, i: 40, train_loss: 0.0825, test_loss: 1.75\n",
      "Epoch: 40, i: 60, train_loss: 0.0679, test_loss: 1.73\n",
      "Epoch: 41, i: 0, train_loss: 0.128, test_loss: 1.74\n",
      "Epoch: 41, i: 20, train_loss: 0.069, test_loss: 1.77\n",
      "Epoch: 41, i: 40, train_loss: 0.0209, test_loss: 1.71\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-39-503a70fe16d3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mhidden\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit_hidden\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_padded\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0mhidden\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_hidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_padded\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_lens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhidden\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m         \u001B[0mtest_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest_loss\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-38-ab57e90755a8>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, x_lens, hidden)\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0membeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0mpacked_embeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpack_padded_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_lens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_first\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menforce_sorted\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0mpacked_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpacked_embeddings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m         \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpad_packed_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpacked_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_first\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0mout_forward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx_lens\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    693\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    694\u001B[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001B[0;32m--> 695\u001B[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001B[0m\u001B[1;32m    696\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m         \u001B[0mhidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#zapis\n",
    "import pickle\n",
    "from google.colab import files\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pk')\n",
    "\n",
    "files.download('model.pk')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zOqKZgWaFI_N",
    "outputId": "4c8c39d7-a4ae-4467-a40a-1b899ef36a82"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/javascript": [
       "download(\"download_22c7c8a8-e098-49cd-bdcf-5f868a99d7dd\", \"model.pk\", 3752287)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TEST"
   ],
   "metadata": {
    "id": "ZkFiCr_t1mtS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_accuracy(test_loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for x, labels in test_loader:\n",
    "    #         x = x.to(device).unsqueeze(2)\n",
    "    #         hidden = model.init_hidden(x.size(0))\n",
    "    #         hidden = hidden.to(device)\n",
    "    #         out, _ = model(x.float(), hidden)\n",
    "    #         out = out.cpu()\n",
    "    #         _, predicted = torch.max(out.data, 1)\n",
    "    #         total += labels.size(0)\n",
    "    #         correct += (predicted == labels).sum().item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_padded, x_lens, y in test_loader:\n",
    "            x_padded, x_lens, y = x_padded.to(torch.int64), x_lens.to(torch.int64), y.to(torch.int64)\n",
    "            x_padded, y = x_padded.to(device), y.to(device)\n",
    "            hidden, state = model.init_hidden(x_padded.size(0))\n",
    "            hidden, state = hidden.to(device), state.to(device)\n",
    "\n",
    "            out, last_hidden = model(x_padded, x_lens, (hidden, state))\n",
    "            out = [i == max(i) for i in out]\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                total += 1\n",
    "                if out[i][y[i]]:\n",
    "                    correct += 1\n",
    "\n",
    "    return 100 * correct / total"
   ],
   "metadata": {
    "id": "dcs1CgxN2QFY"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "measure_accuracy(test_loader, model, device)"
   ],
   "metadata": {
    "id": "nUGvAQKcCyr8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45b33e98-a569-42fb-871a-13f46157692e"
   },
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "81.40589569160997"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model answers"
   ],
   "metadata": {
    "id": "JZXCqbWLPMZC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def test_pad_collate(batch):\n",
    "  xx = batch\n",
    "  x_lens = Tensor([len(x) for x in xx])\n",
    "\n",
    "  xx = [Tensor(i) for i in xx]\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, x_lens"
   ],
   "metadata": {
    "id": "mo6S7sBXRZ8j"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_file = open('test_no_target.pkl', 'rb')\n",
    "test_dataset = pickle.load(test_file)\n",
    "test_dataset = [(i+1) for i in test_dataset]"
   ],
   "metadata": {
    "id": "5jtDqYxD2LSW"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_not_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_pad_collate)"
   ],
   "metadata": {
    "id": "yo3TDnyCQECw"
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "outs = list()\n",
    "with torch.no_grad():\n",
    "        for x_padded, x_lens in test_not_data_loader:\n",
    "            x_padded, x_lens = x_padded.to(torch.int64), x_lens.to(torch.int64)\n",
    "            x_padded = x_padded.to(device)\n",
    "            hidden, state = model.init_hidden(x_padded.size(0))\n",
    "            hidden, state = hidden.to(device), state.to(device)\n",
    "            out, last_hidden = model(x_padded, x_lens, (hidden,state))\n",
    "\n",
    "            for i in out:\n",
    "              outs.append(i.tolist())"
   ],
   "metadata": {
    "id": "UcTBDdPsQRHp"
   },
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "outs = Tensor(outs).argmax(1)\n"
   ],
   "metadata": {
    "id": "KHP2sjtfQSTE"
   },
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "outs_n = outs.numpy()\n",
    "outs_n = pd.DataFrame(outs_n)\n",
    "outs_n.to_csv('sol.csv', index=False, header=False)"
   ],
   "metadata": {
    "id": "lHyoFa1dY3jJ"
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "outs_n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2268
    },
    "id": "bkpSAF4_Zr6E",
    "outputId": "916c95aa-7dc8-4e13-fcc7-4aa78d490863"
   },
   "execution_count": 100,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6b2a010a-5503-4bd5-838e-88f0dad2e54c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1103 rows Ã— 1 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b2a010a-5503-4bd5-838e-88f0dad2e54c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-6b2a010a-5503-4bd5-838e-88f0dad2e54c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-6b2a010a-5503-4bd5-838e-88f0dad2e54c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "      0\n",
       "0     0\n",
       "1     1\n",
       "2     3\n",
       "3     3\n",
       "4     0\n",
       "...  ..\n",
       "1098  2\n",
       "1099  0\n",
       "1100  1\n",
       "1101  0\n",
       "1102  0\n",
       "\n",
       "[1103 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "6bPKLMK_axDf"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}