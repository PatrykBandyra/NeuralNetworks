{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np"
   ],
   "execution_count": 185,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform operations on GPU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "dataframe_raw = pd.read_csv('data.csv', delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explore data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n0        1  2011-01-01       1   0     1   0        0        6           0   \n1        2  2011-01-01       1   0     1   1        0        6           0   \n2        3  2011-01-01       1   0     1   2        0        6           0   \n3        4  2011-01-01       1   0     1   3        0        6           0   \n4        5  2011-01-01       1   0     1   4        0        6           0   \n\n   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n0           1  0.24  0.2879  0.81        0.0       3          13   16  \n1           1  0.22  0.2727  0.80        0.0       8          32   40  \n2           1  0.22  0.2727  0.80        0.0       5          27   32  \n3           1  0.24  0.2879  0.75        0.0       3          10   13  \n4           1  0.24  0.2879  0.75        0.0       0           1    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>dteday</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>casual</th>\n      <th>registered</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.81</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>13</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>32</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>27</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>10</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "   season  yr  mnth  hr  holiday  weekday  workingday  weathersit  temp  \\\n0       1   0     1   0        0        6           0           1  0.24   \n1       1   0     1   1        0        6           0           1  0.22   \n2       1   0     1   2        0        6           0           1  0.22   \n3       1   0     1   3        0        6           0           1  0.24   \n4       1   0     1   4        0        6           0           1  0.24   \n\n    atemp   hum  windspeed  cnt  \n0  0.2879  0.81        0.0   16  \n1  0.2727  0.80        0.0   40  \n2  0.2727  0.80        0.0   32  \n3  0.2879  0.75        0.0   13  \n4  0.2879  0.75        0.0    1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.81</td>\n      <td>0.0</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw = dataframe_raw.drop(['dteday', 'instant', 'casual', 'registered'], axis=1)\n",
    "dataframe_raw.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      10886 non-null  int64  \n",
      " 1   yr          10886 non-null  int64  \n",
      " 2   mnth        10886 non-null  int64  \n",
      " 3   hr          10886 non-null  int64  \n",
      " 4   holiday     10886 non-null  int64  \n",
      " 5   weekday     10886 non-null  int64  \n",
      " 6   workingday  10886 non-null  int64  \n",
      " 7   weathersit  10886 non-null  int64  \n",
      " 8   temp        10886 non-null  float64\n",
      " 9   atemp       10886 non-null  float64\n",
      " 10  hum         10886 non-null  float64\n",
      " 11  windspeed   10886 non-null  float64\n",
      " 12  cnt         10886 non-null  int64  \n",
      "dtypes: float64(4), int64(9)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "dataframe_raw.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "             season            yr          mnth            hr       holiday  \\\ncount  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \nmean       2.506614      0.501929      6.521495     11.541613      0.028569   \nstd        1.116174      0.500019      3.444373      6.915838      0.166599   \nmin        1.000000      0.000000      1.000000      0.000000      0.000000   \n25%        2.000000      0.000000      4.000000      6.000000      0.000000   \n50%        3.000000      1.000000      7.000000     12.000000      0.000000   \n75%        4.000000      1.000000     10.000000     18.000000      0.000000   \nmax        4.000000      1.000000     12.000000     23.000000      1.000000   \n\n            weekday    workingday    weathersit          temp         atemp  \\\ncount  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \nmean       2.998622      0.680875      1.418427      0.493436      0.473102   \nstd        2.007770      0.466159      0.633839      0.190039      0.169492   \nmin        0.000000      0.000000      1.000000      0.020000      0.015200   \n25%        1.000000      0.000000      1.000000      0.340000      0.333300   \n50%        3.000000      1.000000      1.000000      0.500000      0.484800   \n75%        5.000000      1.000000      2.000000      0.640000      0.621200   \nmax        6.000000      1.000000      4.000000      1.000000      0.909100   \n\n                hum     windspeed           cnt  \ncount  10886.000000  10886.000000  10886.000000  \nmean       0.618865      0.191036    191.574132  \nstd        0.192450      0.121859    181.144454  \nmin        0.000000      0.000000      1.000000  \n25%        0.470000      0.104500     42.000000  \n50%        0.620000      0.194000    145.000000  \n75%        0.770000      0.253700    284.000000  \nmax        1.000000      0.850700    977.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n      <td>10886.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.506614</td>\n      <td>0.501929</td>\n      <td>6.521495</td>\n      <td>11.541613</td>\n      <td>0.028569</td>\n      <td>2.998622</td>\n      <td>0.680875</td>\n      <td>1.418427</td>\n      <td>0.493436</td>\n      <td>0.473102</td>\n      <td>0.618865</td>\n      <td>0.191036</td>\n      <td>191.574132</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.116174</td>\n      <td>0.500019</td>\n      <td>3.444373</td>\n      <td>6.915838</td>\n      <td>0.166599</td>\n      <td>2.007770</td>\n      <td>0.466159</td>\n      <td>0.633839</td>\n      <td>0.190039</td>\n      <td>0.169492</td>\n      <td>0.192450</td>\n      <td>0.121859</td>\n      <td>181.144454</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.020000</td>\n      <td>0.015200</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>6.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.340000</td>\n      <td>0.333300</td>\n      <td>0.470000</td>\n      <td>0.104500</td>\n      <td>42.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>7.000000</td>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.484800</td>\n      <td>0.620000</td>\n      <td>0.194000</td>\n      <td>145.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>10.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.640000</td>\n      <td>0.621200</td>\n      <td>0.770000</td>\n      <td>0.253700</td>\n      <td>284.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>12.000000</td>\n      <td>23.000000</td>\n      <td>1.000000</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>0.909100</td>\n      <td>1.000000</td>\n      <td>0.850700</td>\n      <td>977.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "data": {
      "text/plain": "              season        yr      mnth        hr   holiday   weekday  \\\nseason      1.000000 -0.004797  0.971524 -0.006546  0.029368 -0.009691   \nyr         -0.004797  1.000000 -0.004932 -0.004234  0.012021 -0.018162   \nmnth        0.971524 -0.004932  1.000000 -0.006818  0.001731 -0.001012   \nhr         -0.006546 -0.004234 -0.006818  1.000000 -0.000354 -0.001660   \nholiday     0.029368  0.012021  0.001731 -0.000354  1.000000 -0.104800   \nweekday    -0.009691 -0.018162 -0.001012 -0.001660 -0.104800  1.000000   \nworkingday -0.008126 -0.002482 -0.003394  0.002780 -0.250491  0.035554   \nweathersit  0.008879 -0.012548  0.012144 -0.022740 -0.007074 -0.016439   \ntemp        0.258689  0.061226  0.257589  0.145430  0.000295 -0.026143   \natemp       0.264744  0.058540  0.264173  0.140343 -0.005215 -0.035079   \nhum         0.190610 -0.078606  0.204537 -0.278011  0.001929 -0.059722   \nwindspeed  -0.147121 -0.015221 -0.150192  0.146631  0.008409 -0.006178   \ncnt         0.163439  0.260403  0.166862  0.400601 -0.005393  0.027690   \n\n            workingday  weathersit      temp     atemp       hum  windspeed  \\\nseason       -0.008126    0.008879  0.258689  0.264744  0.190610  -0.147121   \nyr           -0.002482   -0.012548  0.061226  0.058540 -0.078606  -0.015221   \nmnth         -0.003394    0.012144  0.257589  0.264173  0.204537  -0.150192   \nhr            0.002780   -0.022740  0.145430  0.140343 -0.278011   0.146631   \nholiday      -0.250491   -0.007074  0.000295 -0.005215  0.001929   0.008409   \nweekday       0.035554   -0.016439 -0.026143 -0.035079 -0.059722  -0.006178   \nworkingday    1.000000    0.033772  0.029966  0.024660 -0.010880   0.013373   \nweathersit    0.033772    1.000000 -0.055035 -0.055376  0.406244   0.007261   \ntemp          0.029966   -0.055035  1.000000  0.984948 -0.064949  -0.017852   \natemp         0.024660   -0.055376  0.984948  1.000000 -0.043536  -0.057473   \nhum          -0.010880    0.406244 -0.064949 -0.043536  1.000000  -0.318607   \nwindspeed     0.013373    0.007261 -0.017852 -0.057473 -0.318607   1.000000   \ncnt           0.011594   -0.128655  0.394454  0.389784 -0.317371   0.101369   \n\n                 cnt  \nseason      0.163439  \nyr          0.260403  \nmnth        0.166862  \nhr          0.400601  \nholiday    -0.005393  \nweekday     0.027690  \nworkingday  0.011594  \nweathersit -0.128655  \ntemp        0.394454  \natemp       0.389784  \nhum        -0.317371  \nwindspeed   0.101369  \ncnt         1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>season</th>\n      <td>1.000000</td>\n      <td>-0.004797</td>\n      <td>0.971524</td>\n      <td>-0.006546</td>\n      <td>0.029368</td>\n      <td>-0.009691</td>\n      <td>-0.008126</td>\n      <td>0.008879</td>\n      <td>0.258689</td>\n      <td>0.264744</td>\n      <td>0.190610</td>\n      <td>-0.147121</td>\n      <td>0.163439</td>\n    </tr>\n    <tr>\n      <th>yr</th>\n      <td>-0.004797</td>\n      <td>1.000000</td>\n      <td>-0.004932</td>\n      <td>-0.004234</td>\n      <td>0.012021</td>\n      <td>-0.018162</td>\n      <td>-0.002482</td>\n      <td>-0.012548</td>\n      <td>0.061226</td>\n      <td>0.058540</td>\n      <td>-0.078606</td>\n      <td>-0.015221</td>\n      <td>0.260403</td>\n    </tr>\n    <tr>\n      <th>mnth</th>\n      <td>0.971524</td>\n      <td>-0.004932</td>\n      <td>1.000000</td>\n      <td>-0.006818</td>\n      <td>0.001731</td>\n      <td>-0.001012</td>\n      <td>-0.003394</td>\n      <td>0.012144</td>\n      <td>0.257589</td>\n      <td>0.264173</td>\n      <td>0.204537</td>\n      <td>-0.150192</td>\n      <td>0.166862</td>\n    </tr>\n    <tr>\n      <th>hr</th>\n      <td>-0.006546</td>\n      <td>-0.004234</td>\n      <td>-0.006818</td>\n      <td>1.000000</td>\n      <td>-0.000354</td>\n      <td>-0.001660</td>\n      <td>0.002780</td>\n      <td>-0.022740</td>\n      <td>0.145430</td>\n      <td>0.140343</td>\n      <td>-0.278011</td>\n      <td>0.146631</td>\n      <td>0.400601</td>\n    </tr>\n    <tr>\n      <th>holiday</th>\n      <td>0.029368</td>\n      <td>0.012021</td>\n      <td>0.001731</td>\n      <td>-0.000354</td>\n      <td>1.000000</td>\n      <td>-0.104800</td>\n      <td>-0.250491</td>\n      <td>-0.007074</td>\n      <td>0.000295</td>\n      <td>-0.005215</td>\n      <td>0.001929</td>\n      <td>0.008409</td>\n      <td>-0.005393</td>\n    </tr>\n    <tr>\n      <th>weekday</th>\n      <td>-0.009691</td>\n      <td>-0.018162</td>\n      <td>-0.001012</td>\n      <td>-0.001660</td>\n      <td>-0.104800</td>\n      <td>1.000000</td>\n      <td>0.035554</td>\n      <td>-0.016439</td>\n      <td>-0.026143</td>\n      <td>-0.035079</td>\n      <td>-0.059722</td>\n      <td>-0.006178</td>\n      <td>0.027690</td>\n    </tr>\n    <tr>\n      <th>workingday</th>\n      <td>-0.008126</td>\n      <td>-0.002482</td>\n      <td>-0.003394</td>\n      <td>0.002780</td>\n      <td>-0.250491</td>\n      <td>0.035554</td>\n      <td>1.000000</td>\n      <td>0.033772</td>\n      <td>0.029966</td>\n      <td>0.024660</td>\n      <td>-0.010880</td>\n      <td>0.013373</td>\n      <td>0.011594</td>\n    </tr>\n    <tr>\n      <th>weathersit</th>\n      <td>0.008879</td>\n      <td>-0.012548</td>\n      <td>0.012144</td>\n      <td>-0.022740</td>\n      <td>-0.007074</td>\n      <td>-0.016439</td>\n      <td>0.033772</td>\n      <td>1.000000</td>\n      <td>-0.055035</td>\n      <td>-0.055376</td>\n      <td>0.406244</td>\n      <td>0.007261</td>\n      <td>-0.128655</td>\n    </tr>\n    <tr>\n      <th>temp</th>\n      <td>0.258689</td>\n      <td>0.061226</td>\n      <td>0.257589</td>\n      <td>0.145430</td>\n      <td>0.000295</td>\n      <td>-0.026143</td>\n      <td>0.029966</td>\n      <td>-0.055035</td>\n      <td>1.000000</td>\n      <td>0.984948</td>\n      <td>-0.064949</td>\n      <td>-0.017852</td>\n      <td>0.394454</td>\n    </tr>\n    <tr>\n      <th>atemp</th>\n      <td>0.264744</td>\n      <td>0.058540</td>\n      <td>0.264173</td>\n      <td>0.140343</td>\n      <td>-0.005215</td>\n      <td>-0.035079</td>\n      <td>0.024660</td>\n      <td>-0.055376</td>\n      <td>0.984948</td>\n      <td>1.000000</td>\n      <td>-0.043536</td>\n      <td>-0.057473</td>\n      <td>0.389784</td>\n    </tr>\n    <tr>\n      <th>hum</th>\n      <td>0.190610</td>\n      <td>-0.078606</td>\n      <td>0.204537</td>\n      <td>-0.278011</td>\n      <td>0.001929</td>\n      <td>-0.059722</td>\n      <td>-0.010880</td>\n      <td>0.406244</td>\n      <td>-0.064949</td>\n      <td>-0.043536</td>\n      <td>1.000000</td>\n      <td>-0.318607</td>\n      <td>-0.317371</td>\n    </tr>\n    <tr>\n      <th>windspeed</th>\n      <td>-0.147121</td>\n      <td>-0.015221</td>\n      <td>-0.150192</td>\n      <td>0.146631</td>\n      <td>0.008409</td>\n      <td>-0.006178</td>\n      <td>0.013373</td>\n      <td>0.007261</td>\n      <td>-0.017852</td>\n      <td>-0.057473</td>\n      <td>-0.318607</td>\n      <td>1.000000</td>\n      <td>0.101369</td>\n    </tr>\n    <tr>\n      <th>cnt</th>\n      <td>0.163439</td>\n      <td>0.260403</td>\n      <td>0.166862</td>\n      <td>0.400601</td>\n      <td>-0.005393</td>\n      <td>0.027690</td>\n      <td>0.011594</td>\n      <td>-0.128655</td>\n      <td>0.394454</td>\n      <td>0.389784</td>\n      <td>-0.317371</td>\n      <td>0.101369</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw.describe()\n",
    "dataframe_raw.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "              season        yr      mnth        hr   holiday   weekday  \\\nseason      1.000000 -0.004797  0.971524 -0.006546  0.029368 -0.009691   \nyr         -0.004797  1.000000 -0.004932 -0.004234  0.012021 -0.018162   \nmnth        0.971524 -0.004932  1.000000 -0.006818  0.001731 -0.001012   \nhr         -0.006546 -0.004234 -0.006818  1.000000 -0.000354 -0.001660   \nholiday     0.029368  0.012021  0.001731 -0.000354  1.000000 -0.104800   \nweekday    -0.009691 -0.018162 -0.001012 -0.001660 -0.104800  1.000000   \nworkingday -0.008126 -0.002482 -0.003394  0.002780 -0.250491  0.035554   \nweathersit  0.008879 -0.012548  0.012144 -0.022740 -0.007074 -0.016439   \ntemp        0.258689  0.061226  0.257589  0.145430  0.000295 -0.026143   \natemp       0.264744  0.058540  0.264173  0.140343 -0.005215 -0.035079   \nhum         0.190610 -0.078606  0.204537 -0.278011  0.001929 -0.059722   \nwindspeed  -0.147121 -0.015221 -0.150192  0.146631  0.008409 -0.006178   \ncnt         0.163439  0.260403  0.166862  0.400601 -0.005393  0.027690   \n\n            workingday  weathersit      temp     atemp       hum  windspeed  \\\nseason       -0.008126    0.008879  0.258689  0.264744  0.190610  -0.147121   \nyr           -0.002482   -0.012548  0.061226  0.058540 -0.078606  -0.015221   \nmnth         -0.003394    0.012144  0.257589  0.264173  0.204537  -0.150192   \nhr            0.002780   -0.022740  0.145430  0.140343 -0.278011   0.146631   \nholiday      -0.250491   -0.007074  0.000295 -0.005215  0.001929   0.008409   \nweekday       0.035554   -0.016439 -0.026143 -0.035079 -0.059722  -0.006178   \nworkingday    1.000000    0.033772  0.029966  0.024660 -0.010880   0.013373   \nweathersit    0.033772    1.000000 -0.055035 -0.055376  0.406244   0.007261   \ntemp          0.029966   -0.055035  1.000000  0.984948 -0.064949  -0.017852   \natemp         0.024660   -0.055376  0.984948  1.000000 -0.043536  -0.057473   \nhum          -0.010880    0.406244 -0.064949 -0.043536  1.000000  -0.318607   \nwindspeed     0.013373    0.007261 -0.017852 -0.057473 -0.318607   1.000000   \ncnt           0.011594   -0.128655  0.394454  0.389784 -0.317371   0.101369   \n\n                 cnt  \nseason      0.163439  \nyr          0.260403  \nmnth        0.166862  \nhr          0.400601  \nholiday    -0.005393  \nweekday     0.027690  \nworkingday  0.011594  \nweathersit -0.128655  \ntemp        0.394454  \natemp       0.389784  \nhum        -0.317371  \nwindspeed   0.101369  \ncnt         1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>season</th>\n      <td>1.000000</td>\n      <td>-0.004797</td>\n      <td>0.971524</td>\n      <td>-0.006546</td>\n      <td>0.029368</td>\n      <td>-0.009691</td>\n      <td>-0.008126</td>\n      <td>0.008879</td>\n      <td>0.258689</td>\n      <td>0.264744</td>\n      <td>0.190610</td>\n      <td>-0.147121</td>\n      <td>0.163439</td>\n    </tr>\n    <tr>\n      <th>yr</th>\n      <td>-0.004797</td>\n      <td>1.000000</td>\n      <td>-0.004932</td>\n      <td>-0.004234</td>\n      <td>0.012021</td>\n      <td>-0.018162</td>\n      <td>-0.002482</td>\n      <td>-0.012548</td>\n      <td>0.061226</td>\n      <td>0.058540</td>\n      <td>-0.078606</td>\n      <td>-0.015221</td>\n      <td>0.260403</td>\n    </tr>\n    <tr>\n      <th>mnth</th>\n      <td>0.971524</td>\n      <td>-0.004932</td>\n      <td>1.000000</td>\n      <td>-0.006818</td>\n      <td>0.001731</td>\n      <td>-0.001012</td>\n      <td>-0.003394</td>\n      <td>0.012144</td>\n      <td>0.257589</td>\n      <td>0.264173</td>\n      <td>0.204537</td>\n      <td>-0.150192</td>\n      <td>0.166862</td>\n    </tr>\n    <tr>\n      <th>hr</th>\n      <td>-0.006546</td>\n      <td>-0.004234</td>\n      <td>-0.006818</td>\n      <td>1.000000</td>\n      <td>-0.000354</td>\n      <td>-0.001660</td>\n      <td>0.002780</td>\n      <td>-0.022740</td>\n      <td>0.145430</td>\n      <td>0.140343</td>\n      <td>-0.278011</td>\n      <td>0.146631</td>\n      <td>0.400601</td>\n    </tr>\n    <tr>\n      <th>holiday</th>\n      <td>0.029368</td>\n      <td>0.012021</td>\n      <td>0.001731</td>\n      <td>-0.000354</td>\n      <td>1.000000</td>\n      <td>-0.104800</td>\n      <td>-0.250491</td>\n      <td>-0.007074</td>\n      <td>0.000295</td>\n      <td>-0.005215</td>\n      <td>0.001929</td>\n      <td>0.008409</td>\n      <td>-0.005393</td>\n    </tr>\n    <tr>\n      <th>weekday</th>\n      <td>-0.009691</td>\n      <td>-0.018162</td>\n      <td>-0.001012</td>\n      <td>-0.001660</td>\n      <td>-0.104800</td>\n      <td>1.000000</td>\n      <td>0.035554</td>\n      <td>-0.016439</td>\n      <td>-0.026143</td>\n      <td>-0.035079</td>\n      <td>-0.059722</td>\n      <td>-0.006178</td>\n      <td>0.027690</td>\n    </tr>\n    <tr>\n      <th>workingday</th>\n      <td>-0.008126</td>\n      <td>-0.002482</td>\n      <td>-0.003394</td>\n      <td>0.002780</td>\n      <td>-0.250491</td>\n      <td>0.035554</td>\n      <td>1.000000</td>\n      <td>0.033772</td>\n      <td>0.029966</td>\n      <td>0.024660</td>\n      <td>-0.010880</td>\n      <td>0.013373</td>\n      <td>0.011594</td>\n    </tr>\n    <tr>\n      <th>weathersit</th>\n      <td>0.008879</td>\n      <td>-0.012548</td>\n      <td>0.012144</td>\n      <td>-0.022740</td>\n      <td>-0.007074</td>\n      <td>-0.016439</td>\n      <td>0.033772</td>\n      <td>1.000000</td>\n      <td>-0.055035</td>\n      <td>-0.055376</td>\n      <td>0.406244</td>\n      <td>0.007261</td>\n      <td>-0.128655</td>\n    </tr>\n    <tr>\n      <th>temp</th>\n      <td>0.258689</td>\n      <td>0.061226</td>\n      <td>0.257589</td>\n      <td>0.145430</td>\n      <td>0.000295</td>\n      <td>-0.026143</td>\n      <td>0.029966</td>\n      <td>-0.055035</td>\n      <td>1.000000</td>\n      <td>0.984948</td>\n      <td>-0.064949</td>\n      <td>-0.017852</td>\n      <td>0.394454</td>\n    </tr>\n    <tr>\n      <th>atemp</th>\n      <td>0.264744</td>\n      <td>0.058540</td>\n      <td>0.264173</td>\n      <td>0.140343</td>\n      <td>-0.005215</td>\n      <td>-0.035079</td>\n      <td>0.024660</td>\n      <td>-0.055376</td>\n      <td>0.984948</td>\n      <td>1.000000</td>\n      <td>-0.043536</td>\n      <td>-0.057473</td>\n      <td>0.389784</td>\n    </tr>\n    <tr>\n      <th>hum</th>\n      <td>0.190610</td>\n      <td>-0.078606</td>\n      <td>0.204537</td>\n      <td>-0.278011</td>\n      <td>0.001929</td>\n      <td>-0.059722</td>\n      <td>-0.010880</td>\n      <td>0.406244</td>\n      <td>-0.064949</td>\n      <td>-0.043536</td>\n      <td>1.000000</td>\n      <td>-0.318607</td>\n      <td>-0.317371</td>\n    </tr>\n    <tr>\n      <th>windspeed</th>\n      <td>-0.147121</td>\n      <td>-0.015221</td>\n      <td>-0.150192</td>\n      <td>0.146631</td>\n      <td>0.008409</td>\n      <td>-0.006178</td>\n      <td>0.013373</td>\n      <td>0.007261</td>\n      <td>-0.017852</td>\n      <td>-0.057473</td>\n      <td>-0.318607</td>\n      <td>1.000000</td>\n      <td>0.101369</td>\n    </tr>\n    <tr>\n      <th>cnt</th>\n      <td>0.163439</td>\n      <td>0.260403</td>\n      <td>0.166862</td>\n      <td>0.400601</td>\n      <td>-0.005393</td>\n      <td>0.027690</td>\n      <td>0.011594</td>\n      <td>-0.128655</td>\n      <td>0.394454</td>\n      <td>0.389784</td>\n      <td>-0.317371</td>\n      <td>0.101369</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_raw.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare dataset for training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "data": {
      "text/plain": "(['season',\n  'yr',\n  'mnth',\n  'hr',\n  'holiday',\n  'weekday',\n  'workingday',\n  'weathersit',\n  'temp',\n  'atemp',\n  'hum',\n  'windspeed'],\n ['cnt'])"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = list(dataframe_raw.columns)[:-1]\n",
    "output_cols = ['cnt']\n",
    "input_cols, output_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "def dataframe_to_arrays(dataframe):\n",
    "    # Extract input & outputs as numpy arrays\n",
    "    inputs_array = dataframe[input_cols].to_numpy()\n",
    "    targets_array = dataframe[output_cols].to_numpy()\n",
    "    return inputs_array, targets_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 1.    ,  0.    ,  1.    , ...,  0.2879,  0.81  ,  0.    ],\n        [ 1.    ,  0.    ,  1.    , ...,  0.2727,  0.8   ,  0.    ],\n        [ 1.    ,  0.    ,  1.    , ...,  0.2727,  0.8   ,  0.    ],\n        ...,\n        [ 4.    ,  1.    , 12.    , ...,  0.3182,  0.61  ,  0.2239],\n        [ 4.    ,  1.    , 12.    , ...,  0.3485,  0.61  ,  0.0896],\n        [ 4.    ,  1.    , 12.    , ...,  0.3333,  0.66  ,  0.1343]]),\n array([[ 16],\n        [ 40],\n        [ 32],\n        ...,\n        [168],\n        [129],\n        [ 88]], dtype=int64))"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_array, targets_array = dataframe_to_arrays(dataframe_raw)\n",
    "inputs_array, targets_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert numpy arrays to torch tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1.0000,  0.0000,  1.0000,  ...,  0.2879,  0.8100,  0.0000],\n         [ 1.0000,  0.0000,  1.0000,  ...,  0.2727,  0.8000,  0.0000],\n         [ 1.0000,  0.0000,  1.0000,  ...,  0.2727,  0.8000,  0.0000],\n         ...,\n         [ 4.0000,  1.0000, 12.0000,  ...,  0.3182,  0.6100,  0.2239],\n         [ 4.0000,  1.0000, 12.0000,  ...,  0.3485,  0.6100,  0.0896],\n         [ 4.0000,  1.0000, 12.0000,  ...,  0.3333,  0.6600,  0.1343]]),\n tensor([[ 16.],\n         [ 40.],\n         [ 32.],\n         ...,\n         [168.],\n         [129.],\n         [ 88.]]))"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs_array).type(torch.float32)\n",
    "targets = torch.from_numpy(targets_array).type(torch.float32)\n",
    "inputs, targets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensor and target tensor:    torch.Size([10886, 12]) | torch.Size([10886, 1]) \n",
      "Datatype of input tensor and target tensor: torch.float32 | torch.float32 \n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of input tensor and target tensor:    {inputs.shape} | {targets.shape} ')\n",
    "print(f'Datatype of input tensor and target tensor: {inputs.dtype} | {targets.dtype} ')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert torch tensor to dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataset.TensorDataset at 0x24158db02e0>"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(inputs, targets)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split dataset into 2 parts - training set, validation set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [8460, 2426])\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display first data batch from train set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[ 2.0000,  0.0000,  5.0000,  6.0000,  0.0000,  3.0000,  1.0000,  3.0000,\n",
      "          0.3400,  0.3030,  0.9300,  0.3881],\n",
      "        [ 1.0000,  1.0000,  3.0000, 18.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.5200,  0.5000,  0.2900,  0.2836],\n",
      "        [ 3.0000,  0.0000,  9.0000, 16.0000,  0.0000,  4.0000,  1.0000,  3.0000,\n",
      "          0.6400,  0.5606,  0.9400,  0.2836],\n",
      "        [ 1.0000,  0.0000,  1.0000, 22.0000,  0.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.1400,  0.1515,  0.6900,  0.1343],\n",
      "        [ 1.0000,  0.0000,  2.0000, 18.0000,  0.0000,  3.0000,  1.0000,  1.0000,\n",
      "          0.4000,  0.4091,  0.4000,  0.2239],\n",
      "        [ 4.0000,  1.0000, 11.0000, 23.0000,  0.0000,  0.0000,  0.0000,  2.0000,\n",
      "          0.3000,  0.3030,  0.5600,  0.1642],\n",
      "        [ 3.0000,  1.0000,  7.0000,  1.0000,  0.0000,  0.0000,  0.0000,  2.0000,\n",
      "          0.6800,  0.6364,  0.8900,  0.1343],\n",
      "        [ 1.0000,  1.0000,  3.0000,  3.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.2400,  0.2424,  0.6000,  0.1343],\n",
      "        [ 4.0000,  0.0000, 12.0000,  0.0000,  0.0000,  4.0000,  1.0000,  1.0000,\n",
      "          0.2800,  0.2576,  0.5200,  0.3284],\n",
      "        [ 2.0000,  1.0000,  5.0000,  9.0000,  0.0000,  3.0000,  1.0000,  2.0000,\n",
      "          0.5600,  0.5303,  0.8800,  0.0000],\n",
      "        [ 2.0000,  1.0000,  4.0000, 13.0000,  0.0000,  2.0000,  1.0000,  1.0000,\n",
      "          0.5600,  0.5303,  0.1900,  0.2537],\n",
      "        [ 4.0000,  0.0000, 12.0000, 15.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.3200,  0.3030,  0.4500,  0.2836],\n",
      "        [ 2.0000,  1.0000,  4.0000,  4.0000,  0.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.4000,  0.4091,  0.7600,  0.4925],\n",
      "        [ 1.0000,  0.0000,  2.0000, 20.0000,  0.0000,  5.0000,  1.0000,  1.0000,\n",
      "          0.6000,  0.6212,  0.3100,  0.1940],\n",
      "        [ 1.0000,  1.0000,  3.0000, 22.0000,  0.0000,  1.0000,  1.0000,  2.0000,\n",
      "          0.5600,  0.5303,  0.5600,  0.2239],\n",
      "        [ 1.0000,  0.0000,  2.0000,  1.0000,  0.0000,  2.0000,  1.0000,  1.0000,\n",
      "          0.3000,  0.2424,  0.4200,  0.7761],\n",
      "        [ 3.0000,  0.0000,  8.0000,  2.0000,  0.0000,  4.0000,  1.0000,  1.0000,\n",
      "          0.6600,  0.6212,  0.5400,  0.1343],\n",
      "        [ 2.0000,  1.0000,  4.0000, 18.0000,  0.0000,  6.0000,  0.0000,  1.0000,\n",
      "          0.5400,  0.5152,  0.1800,  0.2985],\n",
      "        [ 4.0000,  1.0000, 10.0000,  0.0000,  0.0000,  5.0000,  1.0000,  2.0000,\n",
      "          0.5600,  0.5303,  0.8300,  0.1045],\n",
      "        [ 2.0000,  0.0000,  5.0000,  4.0000,  0.0000,  5.0000,  1.0000,  1.0000,\n",
      "          0.3600,  0.3636,  0.7100,  0.1045]])\n",
      "targets: tensor([[ 21.],\n",
      "        [389.],\n",
      "        [156.],\n",
      "        [ 20.],\n",
      "        [222.],\n",
      "        [ 53.],\n",
      "        [168.],\n",
      "        [ 66.],\n",
      "        [ 20.],\n",
      "        [315.],\n",
      "        [345.],\n",
      "        [207.],\n",
      "        [  4.],\n",
      "        [124.],\n",
      "        [133.],\n",
      "        [  5.],\n",
      "        [ 11.],\n",
      "        [517.],\n",
      "        [ 56.],\n",
      "        [  1.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_loader:\n",
    "    print(f'inputs: {xb}')\n",
    "    print(f'targets: {yb}')\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input and output sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "(12, 1)"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(input_cols)\n",
    "output_size = len(output_cols)\n",
    "hidden_size1 = 24\n",
    "hidden_size2 = 12\n",
    "input_size, output_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "class ScooterModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.act_fn1 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.act_fn2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.linear1(xb)\n",
    "        out = self.act_fn1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.act_fn2(out)\n",
    "        out = self.linear3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Generate predictions\n",
    "        out = self(inputs)  # Forward called\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out, targets)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Generate predictions\n",
    "        out = self(inputs)\n",
    "        # Calculate loss\n",
    "        loss = F.l1_loss(out, targets)\n",
    "        return {'val_loss': loss.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result, num_epochs):\n",
    "        # Print result every 100th epoch\n",
    "        if (epoch + 1) % 1 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoch [{epoch + 1}] >> val_loss: {result[\"val_loss\"]:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "ScooterModel(\n  (linear1): Linear(in_features=12, out_features=24, bias=True)\n  (act_fn1): Sigmoid()\n  (linear2): Linear(in_features=24, out_features=12, bias=True)\n  (act_fn2): ReLU()\n  (linear3): Linear(in_features=12, out_features=1, bias=True)\n)"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ScooterModel()\n",
    "model.to(device)\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = validate(model, val_loader)\n",
    "        model.epoch_end(epoch, result, epochs)\n",
    "        history.append(result)\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] >> val_loss: 190.5885\n",
      "Epoch [2] >> val_loss: 170.4632\n",
      "Epoch [3] >> val_loss: 129.9832\n",
      "Epoch [4] >> val_loss: 116.6118\n",
      "Epoch [5] >> val_loss: 113.4118\n",
      "Epoch [6] >> val_loss: 111.9218\n",
      "Epoch [7] >> val_loss: 110.6564\n",
      "Epoch [8] >> val_loss: 109.5857\n",
      "Epoch [9] >> val_loss: 108.5644\n",
      "Epoch [10] >> val_loss: 107.6385\n",
      "Epoch [11] >> val_loss: 106.6981\n",
      "Epoch [12] >> val_loss: 105.7569\n",
      "Epoch [13] >> val_loss: 104.8782\n",
      "Epoch [14] >> val_loss: 104.3581\n",
      "Epoch [15] >> val_loss: 103.3065\n",
      "Epoch [16] >> val_loss: 102.4340\n",
      "Epoch [17] >> val_loss: 101.4543\n",
      "Epoch [18] >> val_loss: 100.4528\n",
      "Epoch [19] >> val_loss: 99.5510\n",
      "Epoch [20] >> val_loss: 98.7610\n",
      "Epoch [21] >> val_loss: 97.7206\n",
      "Epoch [22] >> val_loss: 97.1262\n",
      "Epoch [23] >> val_loss: 96.3970\n",
      "Epoch [24] >> val_loss: 96.2055\n",
      "Epoch [25] >> val_loss: 94.0186\n",
      "Epoch [26] >> val_loss: 93.5416\n",
      "Epoch [27] >> val_loss: 92.1328\n",
      "Epoch [28] >> val_loss: 91.5063\n",
      "Epoch [29] >> val_loss: 90.6540\n",
      "Epoch [30] >> val_loss: 90.5100\n",
      "Epoch [31] >> val_loss: 89.5473\n",
      "Epoch [32] >> val_loss: 89.0837\n",
      "Epoch [33] >> val_loss: 88.1117\n",
      "Epoch [34] >> val_loss: 87.3213\n",
      "Epoch [35] >> val_loss: 86.4968\n",
      "Epoch [36] >> val_loss: 85.7795\n",
      "Epoch [37] >> val_loss: 85.8667\n",
      "Epoch [38] >> val_loss: 86.0214\n",
      "Epoch [39] >> val_loss: 85.0856\n",
      "Epoch [40] >> val_loss: 84.4668\n",
      "Epoch [41] >> val_loss: 84.2034\n",
      "Epoch [42] >> val_loss: 84.0251\n",
      "Epoch [43] >> val_loss: 83.8441\n",
      "Epoch [44] >> val_loss: 85.7629\n",
      "Epoch [45] >> val_loss: 83.4235\n",
      "Epoch [46] >> val_loss: 83.1115\n",
      "Epoch [47] >> val_loss: 82.7050\n",
      "Epoch [48] >> val_loss: 82.9195\n",
      "Epoch [49] >> val_loss: 82.0569\n",
      "Epoch [50] >> val_loss: 80.6084\n",
      "Epoch [51] >> val_loss: 80.8139\n",
      "Epoch [52] >> val_loss: 79.9542\n",
      "Epoch [53] >> val_loss: 80.6518\n",
      "Epoch [54] >> val_loss: 79.6073\n",
      "Epoch [55] >> val_loss: 81.6293\n",
      "Epoch [56] >> val_loss: 79.1023\n",
      "Epoch [57] >> val_loss: 78.2162\n",
      "Epoch [58] >> val_loss: 78.5249\n",
      "Epoch [59] >> val_loss: 76.9956\n",
      "Epoch [60] >> val_loss: 76.8981\n",
      "Epoch [61] >> val_loss: 78.8576\n",
      "Epoch [62] >> val_loss: 78.7162\n",
      "Epoch [63] >> val_loss: 78.6103\n",
      "Epoch [64] >> val_loss: 74.7413\n",
      "Epoch [65] >> val_loss: 75.6128\n",
      "Epoch [66] >> val_loss: 77.3937\n",
      "Epoch [67] >> val_loss: 74.5471\n",
      "Epoch [68] >> val_loss: 73.4110\n",
      "Epoch [69] >> val_loss: 72.3759\n",
      "Epoch [70] >> val_loss: 72.0139\n",
      "Epoch [71] >> val_loss: 73.8556\n",
      "Epoch [72] >> val_loss: 74.3184\n",
      "Epoch [73] >> val_loss: 71.3033\n",
      "Epoch [74] >> val_loss: 73.3457\n",
      "Epoch [75] >> val_loss: 69.5788\n",
      "Epoch [76] >> val_loss: 70.7199\n",
      "Epoch [77] >> val_loss: 74.6385\n",
      "Epoch [78] >> val_loss: 67.0184\n",
      "Epoch [79] >> val_loss: 70.0520\n",
      "Epoch [80] >> val_loss: 67.9305\n",
      "Epoch [81] >> val_loss: 66.2792\n",
      "Epoch [82] >> val_loss: 67.2325\n",
      "Epoch [83] >> val_loss: 67.8568\n",
      "Epoch [84] >> val_loss: 66.3962\n",
      "Epoch [85] >> val_loss: 84.3930\n",
      "Epoch [86] >> val_loss: 65.7655\n",
      "Epoch [87] >> val_loss: 67.2176\n",
      "Epoch [88] >> val_loss: 71.2216\n",
      "Epoch [89] >> val_loss: 72.4248\n",
      "Epoch [90] >> val_loss: 74.5851\n",
      "Epoch [91] >> val_loss: 69.9408\n",
      "Epoch [92] >> val_loss: 69.3292\n",
      "Epoch [93] >> val_loss: 63.5412\n",
      "Epoch [94] >> val_loss: 64.7941\n",
      "Epoch [95] >> val_loss: 63.0737\n",
      "Epoch [96] >> val_loss: 65.0709\n",
      "Epoch [97] >> val_loss: 65.5234\n",
      "Epoch [98] >> val_loss: 66.3831\n",
      "Epoch [99] >> val_loss: 63.9534\n",
      "Epoch [100] >> val_loss: 64.7840\n",
      "Epoch [101] >> val_loss: 73.5740\n",
      "Epoch [102] >> val_loss: 64.7017\n",
      "Epoch [103] >> val_loss: 63.3790\n",
      "Epoch [104] >> val_loss: 62.7980\n",
      "Epoch [105] >> val_loss: 63.4824\n",
      "Epoch [106] >> val_loss: 69.4712\n",
      "Epoch [107] >> val_loss: 61.9984\n",
      "Epoch [108] >> val_loss: 61.5248\n",
      "Epoch [109] >> val_loss: 71.3009\n",
      "Epoch [110] >> val_loss: 62.0895\n",
      "Epoch [111] >> val_loss: 61.5506\n",
      "Epoch [112] >> val_loss: 65.9703\n",
      "Epoch [113] >> val_loss: 70.6515\n",
      "Epoch [114] >> val_loss: 64.1430\n",
      "Epoch [115] >> val_loss: 62.3583\n",
      "Epoch [116] >> val_loss: 64.0118\n",
      "Epoch [117] >> val_loss: 60.9308\n",
      "Epoch [118] >> val_loss: 60.5593\n",
      "Epoch [119] >> val_loss: 61.5456\n",
      "Epoch [120] >> val_loss: 62.9572\n",
      "Epoch [121] >> val_loss: 65.8676\n",
      "Epoch [122] >> val_loss: 60.9419\n",
      "Epoch [123] >> val_loss: 62.0771\n",
      "Epoch [124] >> val_loss: 63.9812\n",
      "Epoch [125] >> val_loss: 62.3288\n",
      "Epoch [126] >> val_loss: 70.1562\n",
      "Epoch [127] >> val_loss: 68.8525\n",
      "Epoch [128] >> val_loss: 58.9488\n",
      "Epoch [129] >> val_loss: 65.1562\n",
      "Epoch [130] >> val_loss: 60.9862\n",
      "Epoch [131] >> val_loss: 59.0548\n",
      "Epoch [132] >> val_loss: 59.6826\n",
      "Epoch [133] >> val_loss: 60.6027\n",
      "Epoch [134] >> val_loss: 59.0194\n",
      "Epoch [135] >> val_loss: 66.4150\n",
      "Epoch [136] >> val_loss: 65.4554\n",
      "Epoch [137] >> val_loss: 59.9329\n",
      "Epoch [138] >> val_loss: 61.0259\n",
      "Epoch [139] >> val_loss: 63.7542\n",
      "Epoch [140] >> val_loss: 60.6038\n",
      "Epoch [141] >> val_loss: 59.2265\n",
      "Epoch [142] >> val_loss: 58.1996\n",
      "Epoch [143] >> val_loss: 58.7330\n",
      "Epoch [144] >> val_loss: 59.2699\n",
      "Epoch [145] >> val_loss: 62.4461\n",
      "Epoch [146] >> val_loss: 64.0678\n",
      "Epoch [147] >> val_loss: 63.4568\n",
      "Epoch [148] >> val_loss: 61.7373\n",
      "Epoch [149] >> val_loss: 57.5327\n",
      "Epoch [150] >> val_loss: 68.5895\n",
      "Epoch [151] >> val_loss: 61.6679\n",
      "Epoch [152] >> val_loss: 56.3986\n",
      "Epoch [153] >> val_loss: 60.0536\n",
      "Epoch [154] >> val_loss: 58.4232\n",
      "Epoch [155] >> val_loss: 57.7141\n",
      "Epoch [156] >> val_loss: 56.3002\n",
      "Epoch [157] >> val_loss: 58.7173\n",
      "Epoch [158] >> val_loss: 64.2013\n",
      "Epoch [159] >> val_loss: 60.4799\n",
      "Epoch [160] >> val_loss: 58.9426\n",
      "Epoch [161] >> val_loss: 59.2288\n",
      "Epoch [162] >> val_loss: 58.7153\n",
      "Epoch [163] >> val_loss: 55.7741\n",
      "Epoch [164] >> val_loss: 57.5418\n",
      "Epoch [165] >> val_loss: 60.9848\n",
      "Epoch [166] >> val_loss: 74.3351\n",
      "Epoch [167] >> val_loss: 56.6985\n",
      "Epoch [168] >> val_loss: 62.6958\n",
      "Epoch [169] >> val_loss: 62.5580\n",
      "Epoch [170] >> val_loss: 57.1125\n",
      "Epoch [171] >> val_loss: 58.9663\n",
      "Epoch [172] >> val_loss: 55.2943\n",
      "Epoch [173] >> val_loss: 55.1793\n",
      "Epoch [174] >> val_loss: 58.3241\n",
      "Epoch [175] >> val_loss: 57.0838\n",
      "Epoch [176] >> val_loss: 57.0378\n",
      "Epoch [177] >> val_loss: 54.4531\n",
      "Epoch [178] >> val_loss: 60.3569\n",
      "Epoch [179] >> val_loss: 66.7650\n",
      "Epoch [180] >> val_loss: 53.2452\n",
      "Epoch [181] >> val_loss: 57.7578\n",
      "Epoch [182] >> val_loss: 54.3638\n",
      "Epoch [183] >> val_loss: 55.7748\n",
      "Epoch [184] >> val_loss: 59.4301\n",
      "Epoch [185] >> val_loss: 57.9567\n",
      "Epoch [186] >> val_loss: 53.6355\n",
      "Epoch [187] >> val_loss: 63.2180\n",
      "Epoch [188] >> val_loss: 53.1658\n",
      "Epoch [189] >> val_loss: 54.4695\n",
      "Epoch [190] >> val_loss: 59.8866\n",
      "Epoch [191] >> val_loss: 53.8044\n",
      "Epoch [192] >> val_loss: 56.0439\n",
      "Epoch [193] >> val_loss: 52.6683\n",
      "Epoch [194] >> val_loss: 52.7929\n",
      "Epoch [195] >> val_loss: 58.0761\n",
      "Epoch [196] >> val_loss: 53.8678\n",
      "Epoch [197] >> val_loss: 55.7458\n",
      "Epoch [198] >> val_loss: 55.4178\n",
      "Epoch [199] >> val_loss: 58.1231\n",
      "Epoch [200] >> val_loss: 53.2830\n",
      "Epoch [201] >> val_loss: 51.5172\n",
      "Epoch [202] >> val_loss: 55.8428\n",
      "Epoch [203] >> val_loss: 55.1840\n",
      "Epoch [204] >> val_loss: 53.9078\n",
      "Epoch [205] >> val_loss: 53.3416\n",
      "Epoch [206] >> val_loss: 57.2024\n",
      "Epoch [207] >> val_loss: 56.8739\n",
      "Epoch [208] >> val_loss: 53.1538\n",
      "Epoch [209] >> val_loss: 54.3314\n",
      "Epoch [210] >> val_loss: 53.2813\n",
      "Epoch [211] >> val_loss: 58.7561\n",
      "Epoch [212] >> val_loss: 54.3324\n",
      "Epoch [213] >> val_loss: 55.6318\n",
      "Epoch [214] >> val_loss: 52.8891\n",
      "Epoch [215] >> val_loss: 52.7246\n",
      "Epoch [216] >> val_loss: 50.6591\n",
      "Epoch [217] >> val_loss: 51.0270\n",
      "Epoch [218] >> val_loss: 52.0158\n",
      "Epoch [219] >> val_loss: 54.5171\n",
      "Epoch [220] >> val_loss: 50.5367\n",
      "Epoch [221] >> val_loss: 50.2432\n",
      "Epoch [222] >> val_loss: 57.0455\n",
      "Epoch [223] >> val_loss: 55.5896\n",
      "Epoch [224] >> val_loss: 52.1702\n",
      "Epoch [225] >> val_loss: 49.4519\n",
      "Epoch [226] >> val_loss: 54.7042\n",
      "Epoch [227] >> val_loss: 49.6443\n",
      "Epoch [228] >> val_loss: 56.8501\n",
      "Epoch [229] >> val_loss: 55.8595\n",
      "Epoch [230] >> val_loss: 48.9216\n",
      "Epoch [231] >> val_loss: 49.0941\n",
      "Epoch [232] >> val_loss: 50.5069\n",
      "Epoch [233] >> val_loss: 50.3208\n",
      "Epoch [234] >> val_loss: 51.1084\n",
      "Epoch [235] >> val_loss: 49.7795\n",
      "Epoch [236] >> val_loss: 50.9677\n",
      "Epoch [237] >> val_loss: 53.7853\n",
      "Epoch [238] >> val_loss: 49.1044\n",
      "Epoch [239] >> val_loss: 52.7424\n",
      "Epoch [240] >> val_loss: 51.7371\n",
      "Epoch [241] >> val_loss: 55.9648\n",
      "Epoch [242] >> val_loss: 65.9893\n",
      "Epoch [243] >> val_loss: 55.7602\n",
      "Epoch [244] >> val_loss: 50.2774\n",
      "Epoch [245] >> val_loss: 49.3474\n",
      "Epoch [246] >> val_loss: 49.1216\n",
      "Epoch [247] >> val_loss: 52.8107\n",
      "Epoch [248] >> val_loss: 48.1495\n",
      "Epoch [249] >> val_loss: 47.1522\n",
      "Epoch [250] >> val_loss: 49.8133\n",
      "Epoch [251] >> val_loss: 51.6479\n",
      "Epoch [252] >> val_loss: 56.4640\n",
      "Epoch [253] >> val_loss: 47.4633\n",
      "Epoch [254] >> val_loss: 50.2000\n",
      "Epoch [255] >> val_loss: 53.4378\n",
      "Epoch [256] >> val_loss: 51.3143\n",
      "Epoch [257] >> val_loss: 64.4984\n",
      "Epoch [258] >> val_loss: 48.8372\n",
      "Epoch [259] >> val_loss: 49.2035\n",
      "Epoch [260] >> val_loss: 54.4648\n",
      "Epoch [261] >> val_loss: 50.9095\n",
      "Epoch [262] >> val_loss: 48.2034\n",
      "Epoch [263] >> val_loss: 48.4153\n",
      "Epoch [264] >> val_loss: 48.9475\n",
      "Epoch [265] >> val_loss: 47.6837\n",
      "Epoch [266] >> val_loss: 49.7869\n",
      "Epoch [267] >> val_loss: 52.4529\n",
      "Epoch [268] >> val_loss: 51.2553\n",
      "Epoch [269] >> val_loss: 49.3443\n",
      "Epoch [270] >> val_loss: 46.5454\n",
      "Epoch [271] >> val_loss: 48.4570\n",
      "Epoch [272] >> val_loss: 48.9614\n",
      "Epoch [273] >> val_loss: 47.4566\n",
      "Epoch [274] >> val_loss: 50.4083\n",
      "Epoch [275] >> val_loss: 50.1011\n",
      "Epoch [276] >> val_loss: 49.1005\n",
      "Epoch [277] >> val_loss: 45.9867\n",
      "Epoch [278] >> val_loss: 51.1647\n",
      "Epoch [279] >> val_loss: 46.8187\n",
      "Epoch [280] >> val_loss: 45.5127\n",
      "Epoch [281] >> val_loss: 50.2264\n",
      "Epoch [282] >> val_loss: 46.1814\n",
      "Epoch [283] >> val_loss: 52.5068\n",
      "Epoch [284] >> val_loss: 48.7734\n",
      "Epoch [285] >> val_loss: 52.5667\n",
      "Epoch [286] >> val_loss: 47.5693\n",
      "Epoch [287] >> val_loss: 48.1150\n",
      "Epoch [288] >> val_loss: 47.0016\n",
      "Epoch [289] >> val_loss: 48.0298\n",
      "Epoch [290] >> val_loss: 52.9369\n",
      "Epoch [291] >> val_loss: 46.6883\n",
      "Epoch [292] >> val_loss: 50.9674\n",
      "Epoch [293] >> val_loss: 50.5491\n",
      "Epoch [294] >> val_loss: 46.6029\n",
      "Epoch [295] >> val_loss: 47.5034\n",
      "Epoch [296] >> val_loss: 47.0114\n",
      "Epoch [297] >> val_loss: 46.7143\n",
      "Epoch [298] >> val_loss: 44.2328\n",
      "Epoch [299] >> val_loss: 47.3490\n",
      "Epoch [300] >> val_loss: 49.4681\n",
      "Epoch [301] >> val_loss: 46.8794\n",
      "Epoch [302] >> val_loss: 52.2930\n",
      "Epoch [303] >> val_loss: 46.7345\n",
      "Epoch [304] >> val_loss: 47.8411\n",
      "Epoch [305] >> val_loss: 52.9011\n",
      "Epoch [306] >> val_loss: 47.3106\n",
      "Epoch [307] >> val_loss: 49.7551\n",
      "Epoch [308] >> val_loss: 46.9383\n",
      "Epoch [309] >> val_loss: 48.8617\n",
      "Epoch [310] >> val_loss: 49.9876\n",
      "Epoch [311] >> val_loss: 44.7423\n",
      "Epoch [312] >> val_loss: 50.4145\n",
      "Epoch [313] >> val_loss: 46.7579\n",
      "Epoch [314] >> val_loss: 45.7918\n",
      "Epoch [315] >> val_loss: 48.7799\n",
      "Epoch [316] >> val_loss: 44.8891\n",
      "Epoch [317] >> val_loss: 48.8199\n",
      "Epoch [318] >> val_loss: 49.1641\n",
      "Epoch [319] >> val_loss: 47.0204\n",
      "Epoch [320] >> val_loss: 45.4484\n",
      "Epoch [321] >> val_loss: 48.3209\n",
      "Epoch [322] >> val_loss: 45.7396\n",
      "Epoch [323] >> val_loss: 50.4287\n",
      "Epoch [324] >> val_loss: 46.2310\n",
      "Epoch [325] >> val_loss: 45.7512\n",
      "Epoch [326] >> val_loss: 44.8353\n",
      "Epoch [327] >> val_loss: 44.7026\n",
      "Epoch [328] >> val_loss: 48.6153\n",
      "Epoch [329] >> val_loss: 44.9411\n",
      "Epoch [330] >> val_loss: 49.0570\n",
      "Epoch [331] >> val_loss: 46.5822\n",
      "Epoch [332] >> val_loss: 47.9517\n",
      "Epoch [333] >> val_loss: 46.5632\n",
      "Epoch [334] >> val_loss: 46.4004\n",
      "Epoch [335] >> val_loss: 44.4397\n",
      "Epoch [336] >> val_loss: 43.5325\n",
      "Epoch [337] >> val_loss: 45.4273\n",
      "Epoch [338] >> val_loss: 47.7112\n",
      "Epoch [339] >> val_loss: 45.3605\n",
      "Epoch [340] >> val_loss: 45.6675\n",
      "Epoch [341] >> val_loss: 46.8111\n",
      "Epoch [342] >> val_loss: 45.5072\n",
      "Epoch [343] >> val_loss: 42.9521\n",
      "Epoch [344] >> val_loss: 43.9985\n",
      "Epoch [345] >> val_loss: 45.3292\n",
      "Epoch [346] >> val_loss: 46.1578\n",
      "Epoch [347] >> val_loss: 45.9672\n",
      "Epoch [348] >> val_loss: 43.8586\n",
      "Epoch [349] >> val_loss: 45.2312\n",
      "Epoch [350] >> val_loss: 49.3493\n",
      "Epoch [351] >> val_loss: 45.0296\n",
      "Epoch [352] >> val_loss: 44.5706\n",
      "Epoch [353] >> val_loss: 43.2688\n",
      "Epoch [354] >> val_loss: 44.1617\n",
      "Epoch [355] >> val_loss: 45.0844\n",
      "Epoch [356] >> val_loss: 48.0007\n",
      "Epoch [357] >> val_loss: 44.8900\n",
      "Epoch [358] >> val_loss: 45.9413\n",
      "Epoch [359] >> val_loss: 44.2813\n",
      "Epoch [360] >> val_loss: 44.1171\n",
      "Epoch [361] >> val_loss: 43.0918\n",
      "Epoch [362] >> val_loss: 46.1987\n",
      "Epoch [363] >> val_loss: 43.7333\n",
      "Epoch [364] >> val_loss: 43.7692\n",
      "Epoch [365] >> val_loss: 44.9533\n",
      "Epoch [366] >> val_loss: 41.7804\n",
      "Epoch [367] >> val_loss: 44.7346\n",
      "Epoch [368] >> val_loss: 45.1665\n",
      "Epoch [369] >> val_loss: 42.7543\n",
      "Epoch [370] >> val_loss: 43.9017\n",
      "Epoch [371] >> val_loss: 42.5632\n",
      "Epoch [372] >> val_loss: 43.1178\n",
      "Epoch [373] >> val_loss: 44.4372\n",
      "Epoch [374] >> val_loss: 44.5940\n",
      "Epoch [375] >> val_loss: 42.2868\n",
      "Epoch [376] >> val_loss: 45.0694\n",
      "Epoch [377] >> val_loss: 46.5044\n",
      "Epoch [378] >> val_loss: 44.6947\n",
      "Epoch [379] >> val_loss: 43.0799\n",
      "Epoch [380] >> val_loss: 46.2516\n",
      "Epoch [381] >> val_loss: 43.9072\n",
      "Epoch [382] >> val_loss: 48.3732\n",
      "Epoch [383] >> val_loss: 49.5341\n",
      "Epoch [384] >> val_loss: 42.0456\n",
      "Epoch [385] >> val_loss: 42.0823\n",
      "Epoch [386] >> val_loss: 42.9071\n",
      "Epoch [387] >> val_loss: 43.2121\n",
      "Epoch [388] >> val_loss: 45.3783\n",
      "Epoch [389] >> val_loss: 42.1996\n",
      "Epoch [390] >> val_loss: 42.3628\n",
      "Epoch [391] >> val_loss: 46.2804\n",
      "Epoch [392] >> val_loss: 43.9632\n",
      "Epoch [393] >> val_loss: 44.2985\n",
      "Epoch [394] >> val_loss: 42.3275\n",
      "Epoch [395] >> val_loss: 42.6253\n",
      "Epoch [396] >> val_loss: 44.7727\n",
      "Epoch [397] >> val_loss: 43.5229\n",
      "Epoch [398] >> val_loss: 44.2215\n",
      "Epoch [399] >> val_loss: 43.0186\n",
      "Epoch [400] >> val_loss: 44.3714\n",
      "Epoch [401] >> val_loss: 46.2004\n",
      "Epoch [402] >> val_loss: 43.3604\n",
      "Epoch [403] >> val_loss: 42.1518\n",
      "Epoch [404] >> val_loss: 44.5995\n",
      "Epoch [405] >> val_loss: 45.4286\n",
      "Epoch [406] >> val_loss: 44.6350\n",
      "Epoch [407] >> val_loss: 41.9161\n",
      "Epoch [408] >> val_loss: 47.6947\n",
      "Epoch [409] >> val_loss: 43.9745\n",
      "Epoch [410] >> val_loss: 46.3756\n",
      "Epoch [411] >> val_loss: 47.2188\n",
      "Epoch [412] >> val_loss: 46.6853\n",
      "Epoch [413] >> val_loss: 46.0169\n",
      "Epoch [414] >> val_loss: 45.8894\n",
      "Epoch [415] >> val_loss: 41.5352\n",
      "Epoch [416] >> val_loss: 43.2160\n",
      "Epoch [417] >> val_loss: 42.6200\n",
      "Epoch [418] >> val_loss: 46.3330\n",
      "Epoch [419] >> val_loss: 42.6150\n",
      "Epoch [420] >> val_loss: 42.5395\n",
      "Epoch [421] >> val_loss: 53.0515\n",
      "Epoch [422] >> val_loss: 44.0957\n",
      "Epoch [423] >> val_loss: 44.2942\n",
      "Epoch [424] >> val_loss: 46.4992\n",
      "Epoch [425] >> val_loss: 43.3645\n",
      "Epoch [426] >> val_loss: 41.8738\n",
      "Epoch [427] >> val_loss: 46.2136\n",
      "Epoch [428] >> val_loss: 41.2806\n",
      "Epoch [429] >> val_loss: 45.6021\n",
      "Epoch [430] >> val_loss: 45.2221\n",
      "Epoch [431] >> val_loss: 43.0546\n",
      "Epoch [432] >> val_loss: 55.6027\n",
      "Epoch [433] >> val_loss: 41.7193\n",
      "Epoch [434] >> val_loss: 42.2319\n",
      "Epoch [435] >> val_loss: 47.7364\n",
      "Epoch [436] >> val_loss: 44.5360\n",
      "Epoch [437] >> val_loss: 44.3870\n",
      "Epoch [438] >> val_loss: 43.6315\n",
      "Epoch [439] >> val_loss: 45.0725\n",
      "Epoch [440] >> val_loss: 42.1730\n",
      "Epoch [441] >> val_loss: 44.8691\n",
      "Epoch [442] >> val_loss: 43.7040\n",
      "Epoch [443] >> val_loss: 43.2156\n",
      "Epoch [444] >> val_loss: 41.9259\n",
      "Epoch [445] >> val_loss: 41.1493\n",
      "Epoch [446] >> val_loss: 43.9243\n",
      "Epoch [447] >> val_loss: 49.3619\n",
      "Epoch [448] >> val_loss: 41.6656\n",
      "Epoch [449] >> val_loss: 44.3081\n",
      "Epoch [450] >> val_loss: 46.3118\n",
      "Epoch [451] >> val_loss: 42.8960\n",
      "Epoch [452] >> val_loss: 41.5896\n",
      "Epoch [453] >> val_loss: 44.6782\n",
      "Epoch [454] >> val_loss: 45.3431\n",
      "Epoch [455] >> val_loss: 44.0064\n",
      "Epoch [456] >> val_loss: 43.2436\n",
      "Epoch [457] >> val_loss: 42.6146\n",
      "Epoch [458] >> val_loss: 40.6456\n",
      "Epoch [459] >> val_loss: 42.0275\n",
      "Epoch [460] >> val_loss: 43.6643\n",
      "Epoch [461] >> val_loss: 45.2544\n",
      "Epoch [462] >> val_loss: 44.0544\n",
      "Epoch [463] >> val_loss: 41.6796\n",
      "Epoch [464] >> val_loss: 42.9243\n",
      "Epoch [465] >> val_loss: 45.7116\n",
      "Epoch [466] >> val_loss: 42.4104\n",
      "Epoch [467] >> val_loss: 41.4188\n",
      "Epoch [468] >> val_loss: 46.8721\n",
      "Epoch [469] >> val_loss: 41.5773\n",
      "Epoch [470] >> val_loss: 44.2649\n",
      "Epoch [471] >> val_loss: 43.7662\n",
      "Epoch [472] >> val_loss: 39.7961\n",
      "Epoch [473] >> val_loss: 40.9621\n",
      "Epoch [474] >> val_loss: 43.1194\n",
      "Epoch [475] >> val_loss: 43.6122\n",
      "Epoch [476] >> val_loss: 44.6716\n",
      "Epoch [477] >> val_loss: 43.9838\n",
      "Epoch [478] >> val_loss: 43.8004\n",
      "Epoch [479] >> val_loss: 45.2918\n",
      "Epoch [480] >> val_loss: 46.1733\n",
      "Epoch [481] >> val_loss: 42.4607\n",
      "Epoch [482] >> val_loss: 41.1758\n",
      "Epoch [483] >> val_loss: 42.3338\n",
      "Epoch [484] >> val_loss: 43.6623\n",
      "Epoch [485] >> val_loss: 48.9012\n",
      "Epoch [486] >> val_loss: 40.3449\n",
      "Epoch [487] >> val_loss: 40.8251\n",
      "Epoch [488] >> val_loss: 43.4740\n",
      "Epoch [489] >> val_loss: 44.4435\n",
      "Epoch [490] >> val_loss: 40.7149\n",
      "Epoch [491] >> val_loss: 43.4628\n",
      "Epoch [492] >> val_loss: 41.5453\n",
      "Epoch [493] >> val_loss: 43.1924\n",
      "Epoch [494] >> val_loss: 44.4387\n",
      "Epoch [495] >> val_loss: 42.5101\n",
      "Epoch [496] >> val_loss: 40.5009\n",
      "Epoch [497] >> val_loss: 40.3125\n",
      "Epoch [498] >> val_loss: 41.5970\n",
      "Epoch [499] >> val_loss: 41.8266\n",
      "Epoch [500] >> val_loss: 41.3940\n",
      "Epoch [501] >> val_loss: 42.4334\n",
      "Epoch [502] >> val_loss: 46.5661\n",
      "Epoch [503] >> val_loss: 42.4134\n",
      "Epoch [504] >> val_loss: 43.5215\n",
      "Epoch [505] >> val_loss: 40.7836\n",
      "Epoch [506] >> val_loss: 41.0514\n",
      "Epoch [507] >> val_loss: 44.5752\n",
      "Epoch [508] >> val_loss: 43.0331\n",
      "Epoch [509] >> val_loss: 41.5557\n",
      "Epoch [510] >> val_loss: 42.3622\n",
      "Epoch [511] >> val_loss: 43.2684\n",
      "Epoch [512] >> val_loss: 41.2946\n",
      "Epoch [513] >> val_loss: 41.4722\n",
      "Epoch [514] >> val_loss: 43.1122\n",
      "Epoch [515] >> val_loss: 42.5118\n",
      "Epoch [516] >> val_loss: 43.0890\n",
      "Epoch [517] >> val_loss: 41.5273\n",
      "Epoch [518] >> val_loss: 41.5883\n",
      "Epoch [519] >> val_loss: 44.4875\n",
      "Epoch [520] >> val_loss: 41.1614\n",
      "Epoch [521] >> val_loss: 44.6861\n",
      "Epoch [522] >> val_loss: 40.0116\n",
      "Epoch [523] >> val_loss: 45.4826\n",
      "Epoch [524] >> val_loss: 41.5054\n",
      "Epoch [525] >> val_loss: 41.1585\n",
      "Epoch [526] >> val_loss: 42.3465\n",
      "Epoch [527] >> val_loss: 42.7721\n",
      "Epoch [528] >> val_loss: 43.1801\n",
      "Epoch [529] >> val_loss: 43.9443\n",
      "Epoch [530] >> val_loss: 42.7813\n",
      "Epoch [531] >> val_loss: 46.6343\n",
      "Epoch [532] >> val_loss: 43.7928\n",
      "Epoch [533] >> val_loss: 39.8966\n",
      "Epoch [534] >> val_loss: 45.7107\n",
      "Epoch [535] >> val_loss: 41.8347\n",
      "Epoch [536] >> val_loss: 40.7936\n",
      "Epoch [537] >> val_loss: 46.5309\n",
      "Epoch [538] >> val_loss: 48.0677\n",
      "Epoch [539] >> val_loss: 46.1623\n",
      "Epoch [540] >> val_loss: 42.4339\n",
      "Epoch [541] >> val_loss: 45.4709\n",
      "Epoch [542] >> val_loss: 41.5930\n",
      "Epoch [543] >> val_loss: 42.5658\n",
      "Epoch [544] >> val_loss: 41.9693\n",
      "Epoch [545] >> val_loss: 43.4558\n",
      "Epoch [546] >> val_loss: 40.6584\n",
      "Epoch [547] >> val_loss: 44.7680\n",
      "Epoch [548] >> val_loss: 40.5697\n",
      "Epoch [549] >> val_loss: 50.4836\n",
      "Epoch [550] >> val_loss: 43.2906\n",
      "Epoch [551] >> val_loss: 41.6636\n",
      "Epoch [552] >> val_loss: 40.5198\n",
      "Epoch [553] >> val_loss: 41.0114\n",
      "Epoch [554] >> val_loss: 39.6208\n",
      "Epoch [555] >> val_loss: 43.6828\n",
      "Epoch [556] >> val_loss: 41.4247\n",
      "Epoch [557] >> val_loss: 44.0900\n",
      "Epoch [558] >> val_loss: 48.2199\n",
      "Epoch [559] >> val_loss: 43.0950\n",
      "Epoch [560] >> val_loss: 49.5526\n",
      "Epoch [561] >> val_loss: 41.5529\n",
      "Epoch [562] >> val_loss: 43.4153\n",
      "Epoch [563] >> val_loss: 42.9666\n",
      "Epoch [564] >> val_loss: 43.6221\n",
      "Epoch [565] >> val_loss: 41.1452\n",
      "Epoch [566] >> val_loss: 42.4847\n",
      "Epoch [567] >> val_loss: 40.1925\n",
      "Epoch [568] >> val_loss: 40.3562\n",
      "Epoch [569] >> val_loss: 41.6860\n",
      "Epoch [570] >> val_loss: 43.5035\n",
      "Epoch [571] >> val_loss: 44.8480\n",
      "Epoch [572] >> val_loss: 39.1914\n",
      "Epoch [573] >> val_loss: 41.7562\n",
      "Epoch [574] >> val_loss: 41.0797\n",
      "Epoch [575] >> val_loss: 43.3568\n",
      "Epoch [576] >> val_loss: 41.6531\n",
      "Epoch [577] >> val_loss: 39.9254\n",
      "Epoch [578] >> val_loss: 41.5404\n",
      "Epoch [579] >> val_loss: 42.9035\n",
      "Epoch [580] >> val_loss: 44.0430\n",
      "Epoch [581] >> val_loss: 40.6003\n",
      "Epoch [582] >> val_loss: 42.7131\n",
      "Epoch [583] >> val_loss: 43.0707\n",
      "Epoch [584] >> val_loss: 40.3300\n",
      "Epoch [585] >> val_loss: 44.9993\n",
      "Epoch [586] >> val_loss: 40.5690\n",
      "Epoch [587] >> val_loss: 45.3675\n",
      "Epoch [588] >> val_loss: 41.2538\n",
      "Epoch [589] >> val_loss: 41.1929\n",
      "Epoch [590] >> val_loss: 42.2876\n",
      "Epoch [591] >> val_loss: 44.3675\n",
      "Epoch [592] >> val_loss: 45.5715\n",
      "Epoch [593] >> val_loss: 39.3758\n",
      "Epoch [594] >> val_loss: 39.1858\n",
      "Epoch [595] >> val_loss: 48.2296\n",
      "Epoch [596] >> val_loss: 42.9912\n",
      "Epoch [597] >> val_loss: 39.4711\n",
      "Epoch [598] >> val_loss: 44.1379\n",
      "Epoch [599] >> val_loss: 40.8897\n",
      "Epoch [600] >> val_loss: 48.4793\n",
      "Epoch [601] >> val_loss: 40.1339\n",
      "Epoch [602] >> val_loss: 45.3341\n",
      "Epoch [603] >> val_loss: 42.2421\n",
      "Epoch [604] >> val_loss: 47.2786\n",
      "Epoch [605] >> val_loss: 40.3697\n",
      "Epoch [606] >> val_loss: 41.3579\n",
      "Epoch [607] >> val_loss: 44.1943\n",
      "Epoch [608] >> val_loss: 42.3992\n",
      "Epoch [609] >> val_loss: 41.1290\n",
      "Epoch [610] >> val_loss: 39.0071\n",
      "Epoch [611] >> val_loss: 40.7963\n",
      "Epoch [612] >> val_loss: 42.8204\n",
      "Epoch [613] >> val_loss: 39.9801\n",
      "Epoch [614] >> val_loss: 44.1587\n",
      "Epoch [615] >> val_loss: 41.6412\n",
      "Epoch [616] >> val_loss: 40.1051\n",
      "Epoch [617] >> val_loss: 42.2841\n",
      "Epoch [618] >> val_loss: 42.5474\n",
      "Epoch [619] >> val_loss: 40.0501\n",
      "Epoch [620] >> val_loss: 42.2997\n",
      "Epoch [621] >> val_loss: 42.7789\n",
      "Epoch [622] >> val_loss: 40.7215\n",
      "Epoch [623] >> val_loss: 41.6428\n",
      "Epoch [624] >> val_loss: 42.6449\n",
      "Epoch [625] >> val_loss: 40.3829\n",
      "Epoch [626] >> val_loss: 41.6924\n",
      "Epoch [627] >> val_loss: 40.9020\n",
      "Epoch [628] >> val_loss: 39.6161\n",
      "Epoch [629] >> val_loss: 42.9074\n",
      "Epoch [630] >> val_loss: 41.9135\n",
      "Epoch [631] >> val_loss: 42.1574\n",
      "Epoch [632] >> val_loss: 38.5274\n",
      "Epoch [633] >> val_loss: 40.6408\n",
      "Epoch [634] >> val_loss: 44.2838\n",
      "Epoch [635] >> val_loss: 41.7363\n",
      "Epoch [636] >> val_loss: 39.7780\n",
      "Epoch [637] >> val_loss: 41.5920\n",
      "Epoch [638] >> val_loss: 45.5401\n",
      "Epoch [639] >> val_loss: 39.2985\n",
      "Epoch [640] >> val_loss: 41.0006\n",
      "Epoch [641] >> val_loss: 41.0309\n",
      "Epoch [642] >> val_loss: 39.9867\n",
      "Epoch [643] >> val_loss: 39.6331\n",
      "Epoch [644] >> val_loss: 42.1214\n",
      "Epoch [645] >> val_loss: 39.6673\n",
      "Epoch [646] >> val_loss: 42.6238\n",
      "Epoch [647] >> val_loss: 45.9242\n",
      "Epoch [648] >> val_loss: 39.4369\n",
      "Epoch [649] >> val_loss: 41.2856\n",
      "Epoch [650] >> val_loss: 40.4011\n",
      "Epoch [651] >> val_loss: 41.0660\n",
      "Epoch [652] >> val_loss: 39.2375\n",
      "Epoch [653] >> val_loss: 41.5991\n",
      "Epoch [654] >> val_loss: 39.3862\n",
      "Epoch [655] >> val_loss: 39.4609\n",
      "Epoch [656] >> val_loss: 39.6183\n",
      "Epoch [657] >> val_loss: 40.6142\n",
      "Epoch [658] >> val_loss: 40.1600\n",
      "Epoch [659] >> val_loss: 41.1636\n",
      "Epoch [660] >> val_loss: 39.9937\n",
      "Epoch [661] >> val_loss: 38.9028\n",
      "Epoch [662] >> val_loss: 40.6535\n",
      "Epoch [663] >> val_loss: 40.1968\n",
      "Epoch [664] >> val_loss: 39.3659\n",
      "Epoch [665] >> val_loss: 39.3164\n",
      "Epoch [666] >> val_loss: 45.9931\n",
      "Epoch [667] >> val_loss: 40.2903\n",
      "Epoch [668] >> val_loss: 39.8912\n",
      "Epoch [669] >> val_loss: 45.9492\n",
      "Epoch [670] >> val_loss: 40.1807\n",
      "Epoch [671] >> val_loss: 38.2251\n",
      "Epoch [672] >> val_loss: 42.7690\n",
      "Epoch [673] >> val_loss: 42.5189\n",
      "Epoch [674] >> val_loss: 42.4227\n",
      "Epoch [675] >> val_loss: 37.8456\n",
      "Epoch [676] >> val_loss: 39.1110\n",
      "Epoch [677] >> val_loss: 39.7256\n",
      "Epoch [678] >> val_loss: 40.0560\n",
      "Epoch [679] >> val_loss: 41.0735\n",
      "Epoch [680] >> val_loss: 40.3808\n",
      "Epoch [681] >> val_loss: 40.4164\n",
      "Epoch [682] >> val_loss: 39.2243\n",
      "Epoch [683] >> val_loss: 42.5071\n",
      "Epoch [684] >> val_loss: 40.4394\n",
      "Epoch [685] >> val_loss: 41.3384\n",
      "Epoch [686] >> val_loss: 41.1147\n",
      "Epoch [687] >> val_loss: 42.9946\n",
      "Epoch [688] >> val_loss: 38.8988\n",
      "Epoch [689] >> val_loss: 39.1838\n",
      "Epoch [690] >> val_loss: 40.5848\n",
      "Epoch [691] >> val_loss: 41.2268\n",
      "Epoch [692] >> val_loss: 38.7958\n",
      "Epoch [693] >> val_loss: 41.0939\n",
      "Epoch [694] >> val_loss: 40.5543\n",
      "Epoch [695] >> val_loss: 39.6456\n",
      "Epoch [696] >> val_loss: 38.5359\n",
      "Epoch [697] >> val_loss: 45.2393\n",
      "Epoch [698] >> val_loss: 43.1385\n",
      "Epoch [699] >> val_loss: 40.8316\n",
      "Epoch [700] >> val_loss: 41.9193\n",
      "Epoch [701] >> val_loss: 43.0574\n",
      "Epoch [702] >> val_loss: 40.9859\n",
      "Epoch [703] >> val_loss: 39.4085\n",
      "Epoch [704] >> val_loss: 38.3656\n",
      "Epoch [705] >> val_loss: 40.4212\n",
      "Epoch [706] >> val_loss: 39.5940\n",
      "Epoch [707] >> val_loss: 39.9459\n",
      "Epoch [708] >> val_loss: 44.5621\n",
      "Epoch [709] >> val_loss: 42.9479\n",
      "Epoch [710] >> val_loss: 42.8346\n",
      "Epoch [711] >> val_loss: 41.4682\n",
      "Epoch [712] >> val_loss: 42.8057\n",
      "Epoch [713] >> val_loss: 38.9801\n",
      "Epoch [714] >> val_loss: 42.3462\n",
      "Epoch [715] >> val_loss: 38.0644\n",
      "Epoch [716] >> val_loss: 39.3641\n",
      "Epoch [717] >> val_loss: 37.5508\n",
      "Epoch [718] >> val_loss: 39.7925\n",
      "Epoch [719] >> val_loss: 41.4603\n",
      "Epoch [720] >> val_loss: 42.9653\n",
      "Epoch [721] >> val_loss: 41.0911\n",
      "Epoch [722] >> val_loss: 41.0982\n",
      "Epoch [723] >> val_loss: 43.6984\n",
      "Epoch [724] >> val_loss: 40.5066\n",
      "Epoch [725] >> val_loss: 40.3088\n",
      "Epoch [726] >> val_loss: 38.9240\n",
      "Epoch [727] >> val_loss: 44.4897\n",
      "Epoch [728] >> val_loss: 40.2248\n",
      "Epoch [729] >> val_loss: 41.4502\n",
      "Epoch [730] >> val_loss: 39.5288\n",
      "Epoch [731] >> val_loss: 39.7021\n",
      "Epoch [732] >> val_loss: 39.3720\n",
      "Epoch [733] >> val_loss: 43.7623\n",
      "Epoch [734] >> val_loss: 37.6704\n",
      "Epoch [735] >> val_loss: 39.9192\n",
      "Epoch [736] >> val_loss: 44.7102\n",
      "Epoch [737] >> val_loss: 37.9020\n",
      "Epoch [738] >> val_loss: 37.5581\n",
      "Epoch [739] >> val_loss: 41.4124\n",
      "Epoch [740] >> val_loss: 37.5859\n",
      "Epoch [741] >> val_loss: 38.3315\n",
      "Epoch [742] >> val_loss: 39.2010\n",
      "Epoch [743] >> val_loss: 40.4887\n",
      "Epoch [744] >> val_loss: 50.9958\n",
      "Epoch [745] >> val_loss: 40.7542\n",
      "Epoch [746] >> val_loss: 40.5466\n",
      "Epoch [747] >> val_loss: 37.2157\n",
      "Epoch [748] >> val_loss: 42.2619\n",
      "Epoch [749] >> val_loss: 40.4984\n",
      "Epoch [750] >> val_loss: 38.6878\n",
      "Epoch [751] >> val_loss: 38.5348\n",
      "Epoch [752] >> val_loss: 39.5020\n",
      "Epoch [753] >> val_loss: 38.8758\n",
      "Epoch [754] >> val_loss: 38.7788\n",
      "Epoch [755] >> val_loss: 39.9776\n",
      "Epoch [756] >> val_loss: 44.4158\n",
      "Epoch [757] >> val_loss: 41.0720\n",
      "Epoch [758] >> val_loss: 40.6485\n",
      "Epoch [759] >> val_loss: 39.3112\n",
      "Epoch [760] >> val_loss: 38.9713\n",
      "Epoch [761] >> val_loss: 42.1290\n",
      "Epoch [762] >> val_loss: 38.3608\n",
      "Epoch [763] >> val_loss: 40.6384\n",
      "Epoch [764] >> val_loss: 40.4810\n",
      "Epoch [765] >> val_loss: 42.6195\n",
      "Epoch [766] >> val_loss: 37.7004\n",
      "Epoch [767] >> val_loss: 44.0887\n",
      "Epoch [768] >> val_loss: 37.7636\n",
      "Epoch [769] >> val_loss: 38.0899\n",
      "Epoch [770] >> val_loss: 42.3170\n",
      "Epoch [771] >> val_loss: 39.6430\n",
      "Epoch [772] >> val_loss: 41.9545\n",
      "Epoch [773] >> val_loss: 38.2993\n",
      "Epoch [774] >> val_loss: 50.4319\n",
      "Epoch [775] >> val_loss: 39.4622\n",
      "Epoch [776] >> val_loss: 37.5315\n",
      "Epoch [777] >> val_loss: 43.1633\n",
      "Epoch [778] >> val_loss: 39.1530\n",
      "Epoch [779] >> val_loss: 37.8848\n",
      "Epoch [780] >> val_loss: 42.8706\n",
      "Epoch [781] >> val_loss: 44.9474\n",
      "Epoch [782] >> val_loss: 41.7812\n",
      "Epoch [783] >> val_loss: 37.5063\n",
      "Epoch [784] >> val_loss: 38.4376\n",
      "Epoch [785] >> val_loss: 41.0442\n",
      "Epoch [786] >> val_loss: 39.1895\n",
      "Epoch [787] >> val_loss: 39.2006\n",
      "Epoch [788] >> val_loss: 42.3690\n",
      "Epoch [789] >> val_loss: 47.8653\n",
      "Epoch [790] >> val_loss: 39.8571\n",
      "Epoch [791] >> val_loss: 40.2728\n",
      "Epoch [792] >> val_loss: 38.3999\n",
      "Epoch [793] >> val_loss: 37.5602\n",
      "Epoch [794] >> val_loss: 41.9527\n",
      "Epoch [795] >> val_loss: 38.5681\n",
      "Epoch [796] >> val_loss: 40.4121\n",
      "Epoch [797] >> val_loss: 38.1595\n",
      "Epoch [798] >> val_loss: 38.5927\n",
      "Epoch [799] >> val_loss: 37.7200\n",
      "Epoch [800] >> val_loss: 39.8363\n",
      "Epoch [801] >> val_loss: 38.0746\n",
      "Epoch [802] >> val_loss: 42.1680\n",
      "Epoch [803] >> val_loss: 38.5165\n",
      "Epoch [804] >> val_loss: 40.5818\n",
      "Epoch [805] >> val_loss: 39.7739\n",
      "Epoch [806] >> val_loss: 38.7886\n",
      "Epoch [807] >> val_loss: 40.1485\n",
      "Epoch [808] >> val_loss: 39.8122\n",
      "Epoch [809] >> val_loss: 38.1759\n",
      "Epoch [810] >> val_loss: 37.5875\n",
      "Epoch [811] >> val_loss: 42.2252\n",
      "Epoch [812] >> val_loss: 38.3326\n",
      "Epoch [813] >> val_loss: 39.4375\n",
      "Epoch [814] >> val_loss: 39.3456\n",
      "Epoch [815] >> val_loss: 40.9259\n",
      "Epoch [816] >> val_loss: 38.9410\n",
      "Epoch [817] >> val_loss: 40.7480\n",
      "Epoch [818] >> val_loss: 38.6310\n",
      "Epoch [819] >> val_loss: 41.7278\n",
      "Epoch [820] >> val_loss: 38.5171\n",
      "Epoch [821] >> val_loss: 40.6526\n",
      "Epoch [822] >> val_loss: 39.6173\n",
      "Epoch [823] >> val_loss: 40.4903\n",
      "Epoch [824] >> val_loss: 37.8779\n",
      "Epoch [825] >> val_loss: 43.9272\n",
      "Epoch [826] >> val_loss: 37.7090\n",
      "Epoch [827] >> val_loss: 36.6502\n",
      "Epoch [828] >> val_loss: 37.2202\n",
      "Epoch [829] >> val_loss: 40.3143\n",
      "Epoch [830] >> val_loss: 39.4008\n",
      "Epoch [831] >> val_loss: 38.4226\n",
      "Epoch [832] >> val_loss: 41.0791\n",
      "Epoch [833] >> val_loss: 36.8702\n",
      "Epoch [834] >> val_loss: 39.7576\n",
      "Epoch [835] >> val_loss: 38.5698\n",
      "Epoch [836] >> val_loss: 40.7378\n",
      "Epoch [837] >> val_loss: 37.8663\n",
      "Epoch [838] >> val_loss: 41.1310\n",
      "Epoch [839] >> val_loss: 37.3125\n",
      "Epoch [840] >> val_loss: 40.9162\n",
      "Epoch [841] >> val_loss: 42.0829\n",
      "Epoch [842] >> val_loss: 44.0549\n",
      "Epoch [843] >> val_loss: 38.7582\n",
      "Epoch [844] >> val_loss: 44.5899\n",
      "Epoch [845] >> val_loss: 38.3633\n",
      "Epoch [846] >> val_loss: 41.7247\n",
      "Epoch [847] >> val_loss: 39.6138\n",
      "Epoch [848] >> val_loss: 37.2807\n",
      "Epoch [849] >> val_loss: 39.0990\n",
      "Epoch [850] >> val_loss: 37.9340\n",
      "Epoch [851] >> val_loss: 39.3238\n",
      "Epoch [852] >> val_loss: 39.3059\n",
      "Epoch [853] >> val_loss: 42.6579\n",
      "Epoch [854] >> val_loss: 39.7651\n",
      "Epoch [855] >> val_loss: 41.7818\n",
      "Epoch [856] >> val_loss: 38.9463\n",
      "Epoch [857] >> val_loss: 43.2941\n",
      "Epoch [858] >> val_loss: 40.2870\n",
      "Epoch [859] >> val_loss: 37.7953\n",
      "Epoch [860] >> val_loss: 39.2441\n",
      "Epoch [861] >> val_loss: 45.6023\n",
      "Epoch [862] >> val_loss: 39.4149\n",
      "Epoch [863] >> val_loss: 39.2187\n",
      "Epoch [864] >> val_loss: 40.2620\n",
      "Epoch [865] >> val_loss: 38.4913\n",
      "Epoch [866] >> val_loss: 40.9841\n",
      "Epoch [867] >> val_loss: 38.3837\n",
      "Epoch [868] >> val_loss: 42.1767\n",
      "Epoch [869] >> val_loss: 41.0203\n",
      "Epoch [870] >> val_loss: 42.0155\n",
      "Epoch [871] >> val_loss: 38.9537\n",
      "Epoch [872] >> val_loss: 42.3108\n",
      "Epoch [873] >> val_loss: 40.0143\n",
      "Epoch [874] >> val_loss: 41.0757\n",
      "Epoch [875] >> val_loss: 40.0574\n",
      "Epoch [876] >> val_loss: 39.5210\n",
      "Epoch [877] >> val_loss: 40.9364\n",
      "Epoch [878] >> val_loss: 37.9131\n",
      "Epoch [879] >> val_loss: 38.7019\n",
      "Epoch [880] >> val_loss: 38.5297\n",
      "Epoch [881] >> val_loss: 36.7125\n",
      "Epoch [882] >> val_loss: 39.5424\n",
      "Epoch [883] >> val_loss: 37.9899\n",
      "Epoch [884] >> val_loss: 40.8324\n",
      "Epoch [885] >> val_loss: 41.0956\n",
      "Epoch [886] >> val_loss: 39.6418\n",
      "Epoch [887] >> val_loss: 36.1876\n",
      "Epoch [888] >> val_loss: 42.7099\n",
      "Epoch [889] >> val_loss: 42.8683\n",
      "Epoch [890] >> val_loss: 38.3238\n",
      "Epoch [891] >> val_loss: 38.3349\n",
      "Epoch [892] >> val_loss: 41.1110\n",
      "Epoch [893] >> val_loss: 42.3218\n",
      "Epoch [894] >> val_loss: 38.7066\n",
      "Epoch [895] >> val_loss: 38.1492\n",
      "Epoch [896] >> val_loss: 44.5292\n",
      "Epoch [897] >> val_loss: 39.1488\n",
      "Epoch [898] >> val_loss: 37.4243\n",
      "Epoch [899] >> val_loss: 39.0214\n",
      "Epoch [900] >> val_loss: 37.9613\n",
      "Epoch [901] >> val_loss: 38.4154\n",
      "Epoch [902] >> val_loss: 38.5205\n",
      "Epoch [903] >> val_loss: 43.1422\n",
      "Epoch [904] >> val_loss: 38.4582\n",
      "Epoch [905] >> val_loss: 41.0455\n",
      "Epoch [906] >> val_loss: 40.3267\n",
      "Epoch [907] >> val_loss: 38.0808\n",
      "Epoch [908] >> val_loss: 36.8894\n",
      "Epoch [909] >> val_loss: 39.3521\n",
      "Epoch [910] >> val_loss: 37.9469\n",
      "Epoch [911] >> val_loss: 38.2268\n",
      "Epoch [912] >> val_loss: 41.9903\n",
      "Epoch [913] >> val_loss: 41.6257\n",
      "Epoch [914] >> val_loss: 38.3928\n",
      "Epoch [915] >> val_loss: 37.4809\n",
      "Epoch [916] >> val_loss: 35.8250\n",
      "Epoch [917] >> val_loss: 41.9530\n",
      "Epoch [918] >> val_loss: 40.4087\n",
      "Epoch [919] >> val_loss: 42.5428\n",
      "Epoch [920] >> val_loss: 44.1115\n",
      "Epoch [921] >> val_loss: 38.1982\n",
      "Epoch [922] >> val_loss: 39.2246\n",
      "Epoch [923] >> val_loss: 35.9984\n",
      "Epoch [924] >> val_loss: 39.3475\n",
      "Epoch [925] >> val_loss: 39.3318\n",
      "Epoch [926] >> val_loss: 38.4806\n",
      "Epoch [927] >> val_loss: 39.0696\n",
      "Epoch [928] >> val_loss: 40.9068\n",
      "Epoch [929] >> val_loss: 36.4240\n",
      "Epoch [930] >> val_loss: 37.3017\n",
      "Epoch [931] >> val_loss: 36.8130\n",
      "Epoch [932] >> val_loss: 37.3157\n",
      "Epoch [933] >> val_loss: 37.0991\n",
      "Epoch [934] >> val_loss: 35.8388\n",
      "Epoch [935] >> val_loss: 38.6604\n",
      "Epoch [936] >> val_loss: 37.2480\n",
      "Epoch [937] >> val_loss: 36.8094\n",
      "Epoch [938] >> val_loss: 47.8666\n",
      "Epoch [939] >> val_loss: 41.5069\n",
      "Epoch [940] >> val_loss: 38.0031\n",
      "Epoch [941] >> val_loss: 38.6647\n",
      "Epoch [942] >> val_loss: 36.6412\n",
      "Epoch [943] >> val_loss: 36.3742\n",
      "Epoch [944] >> val_loss: 38.7136\n",
      "Epoch [945] >> val_loss: 37.1079\n",
      "Epoch [946] >> val_loss: 38.3661\n",
      "Epoch [947] >> val_loss: 40.4267\n",
      "Epoch [948] >> val_loss: 37.6664\n",
      "Epoch [949] >> val_loss: 37.0469\n",
      "Epoch [950] >> val_loss: 41.1956\n",
      "Epoch [951] >> val_loss: 37.8153\n",
      "Epoch [952] >> val_loss: 40.0881\n",
      "Epoch [953] >> val_loss: 38.2326\n",
      "Epoch [954] >> val_loss: 41.0686\n",
      "Epoch [955] >> val_loss: 40.9122\n",
      "Epoch [956] >> val_loss: 35.9331\n",
      "Epoch [957] >> val_loss: 36.1939\n",
      "Epoch [958] >> val_loss: 36.4610\n",
      "Epoch [959] >> val_loss: 38.8015\n",
      "Epoch [960] >> val_loss: 40.6868\n",
      "Epoch [961] >> val_loss: 41.4276\n",
      "Epoch [962] >> val_loss: 40.8515\n",
      "Epoch [963] >> val_loss: 46.9413\n",
      "Epoch [964] >> val_loss: 37.9891\n",
      "Epoch [965] >> val_loss: 36.0916\n",
      "Epoch [966] >> val_loss: 38.6973\n",
      "Epoch [967] >> val_loss: 38.3174\n",
      "Epoch [968] >> val_loss: 37.0558\n",
      "Epoch [969] >> val_loss: 38.0606\n",
      "Epoch [970] >> val_loss: 37.7734\n",
      "Epoch [971] >> val_loss: 38.4466\n",
      "Epoch [972] >> val_loss: 40.3953\n",
      "Epoch [973] >> val_loss: 37.2049\n",
      "Epoch [974] >> val_loss: 36.3459\n",
      "Epoch [975] >> val_loss: 36.9125\n",
      "Epoch [976] >> val_loss: 39.5771\n",
      "Epoch [977] >> val_loss: 37.6243\n",
      "Epoch [978] >> val_loss: 43.4223\n",
      "Epoch [979] >> val_loss: 37.2066\n",
      "Epoch [980] >> val_loss: 39.5994\n",
      "Epoch [981] >> val_loss: 39.3831\n",
      "Epoch [982] >> val_loss: 37.1159\n",
      "Epoch [983] >> val_loss: 41.4490\n",
      "Epoch [984] >> val_loss: 42.4448\n",
      "Epoch [985] >> val_loss: 35.7867\n",
      "Epoch [986] >> val_loss: 35.8213\n",
      "Epoch [987] >> val_loss: 38.1274\n",
      "Epoch [988] >> val_loss: 39.5774\n",
      "Epoch [989] >> val_loss: 36.6603\n",
      "Epoch [990] >> val_loss: 40.2327\n",
      "Epoch [991] >> val_loss: 40.3871\n",
      "Epoch [992] >> val_loss: 36.2186\n",
      "Epoch [993] >> val_loss: 37.3794\n",
      "Epoch [994] >> val_loss: 38.1758\n",
      "Epoch [995] >> val_loss: 37.3462\n",
      "Epoch [996] >> val_loss: 38.6508\n",
      "Epoch [997] >> val_loss: 38.2524\n",
      "Epoch [998] >> val_loss: 36.7190\n",
      "Epoch [999] >> val_loss: 36.4450\n",
      "Epoch [1000] >> val_loss: 38.0451\n",
      "Epoch [1001] >> val_loss: 40.3725\n",
      "Epoch [1002] >> val_loss: 38.2970\n",
      "Epoch [1003] >> val_loss: 36.2073\n",
      "Epoch [1004] >> val_loss: 43.2166\n",
      "Epoch [1005] >> val_loss: 35.2351\n",
      "Epoch [1006] >> val_loss: 39.3872\n",
      "Epoch [1007] >> val_loss: 36.8311\n",
      "Epoch [1008] >> val_loss: 39.6348\n",
      "Epoch [1009] >> val_loss: 37.5224\n",
      "Epoch [1010] >> val_loss: 42.1217\n",
      "Epoch [1011] >> val_loss: 40.4307\n",
      "Epoch [1012] >> val_loss: 42.3279\n",
      "Epoch [1013] >> val_loss: 36.8187\n",
      "Epoch [1014] >> val_loss: 36.3264\n",
      "Epoch [1015] >> val_loss: 42.7943\n",
      "Epoch [1016] >> val_loss: 38.5164\n",
      "Epoch [1017] >> val_loss: 36.5720\n",
      "Epoch [1018] >> val_loss: 38.9981\n",
      "Epoch [1019] >> val_loss: 35.9690\n",
      "Epoch [1020] >> val_loss: 38.2012\n",
      "Epoch [1021] >> val_loss: 37.5865\n",
      "Epoch [1022] >> val_loss: 39.3057\n",
      "Epoch [1023] >> val_loss: 39.6737\n",
      "Epoch [1024] >> val_loss: 35.2086\n",
      "Epoch [1025] >> val_loss: 35.9366\n",
      "Epoch [1026] >> val_loss: 40.7832\n",
      "Epoch [1027] >> val_loss: 37.3799\n",
      "Epoch [1028] >> val_loss: 40.0093\n",
      "Epoch [1029] >> val_loss: 35.7156\n",
      "Epoch [1030] >> val_loss: 38.2639\n",
      "Epoch [1031] >> val_loss: 37.9761\n",
      "Epoch [1032] >> val_loss: 41.0372\n",
      "Epoch [1033] >> val_loss: 38.1321\n",
      "Epoch [1034] >> val_loss: 35.9490\n",
      "Epoch [1035] >> val_loss: 49.1107\n",
      "Epoch [1036] >> val_loss: 36.4154\n",
      "Epoch [1037] >> val_loss: 35.9240\n",
      "Epoch [1038] >> val_loss: 45.0006\n",
      "Epoch [1039] >> val_loss: 38.7461\n",
      "Epoch [1040] >> val_loss: 37.0083\n",
      "Epoch [1041] >> val_loss: 35.4271\n",
      "Epoch [1042] >> val_loss: 37.7498\n",
      "Epoch [1043] >> val_loss: 37.6884\n",
      "Epoch [1044] >> val_loss: 37.5509\n",
      "Epoch [1045] >> val_loss: 38.7104\n",
      "Epoch [1046] >> val_loss: 40.3688\n",
      "Epoch [1047] >> val_loss: 38.3445\n",
      "Epoch [1048] >> val_loss: 36.6389\n",
      "Epoch [1049] >> val_loss: 37.4528\n",
      "Epoch [1050] >> val_loss: 45.5949\n",
      "Epoch [1051] >> val_loss: 35.1312\n",
      "Epoch [1052] >> val_loss: 37.2748\n",
      "Epoch [1053] >> val_loss: 37.0864\n",
      "Epoch [1054] >> val_loss: 40.8434\n",
      "Epoch [1055] >> val_loss: 35.9616\n",
      "Epoch [1056] >> val_loss: 36.7822\n",
      "Epoch [1057] >> val_loss: 35.2440\n",
      "Epoch [1058] >> val_loss: 36.3051\n",
      "Epoch [1059] >> val_loss: 39.7295\n",
      "Epoch [1060] >> val_loss: 39.7881\n",
      "Epoch [1061] >> val_loss: 36.4507\n",
      "Epoch [1062] >> val_loss: 36.0504\n",
      "Epoch [1063] >> val_loss: 39.7690\n",
      "Epoch [1064] >> val_loss: 35.3326\n",
      "Epoch [1065] >> val_loss: 41.0382\n",
      "Epoch [1066] >> val_loss: 39.0912\n",
      "Epoch [1067] >> val_loss: 37.9263\n",
      "Epoch [1068] >> val_loss: 37.6045\n",
      "Epoch [1069] >> val_loss: 41.5286\n",
      "Epoch [1070] >> val_loss: 35.5399\n",
      "Epoch [1071] >> val_loss: 39.0135\n",
      "Epoch [1072] >> val_loss: 36.1039\n",
      "Epoch [1073] >> val_loss: 40.6571\n",
      "Epoch [1074] >> val_loss: 39.2721\n",
      "Epoch [1075] >> val_loss: 37.6002\n",
      "Epoch [1076] >> val_loss: 35.7892\n",
      "Epoch [1077] >> val_loss: 36.0046\n",
      "Epoch [1078] >> val_loss: 36.3473\n",
      "Epoch [1079] >> val_loss: 42.8256\n",
      "Epoch [1080] >> val_loss: 38.2725\n",
      "Epoch [1081] >> val_loss: 35.5069\n",
      "Epoch [1082] >> val_loss: 37.6470\n",
      "Epoch [1083] >> val_loss: 40.9619\n",
      "Epoch [1084] >> val_loss: 37.9137\n",
      "Epoch [1085] >> val_loss: 41.7018\n",
      "Epoch [1086] >> val_loss: 36.9365\n",
      "Epoch [1087] >> val_loss: 42.3057\n",
      "Epoch [1088] >> val_loss: 36.9217\n",
      "Epoch [1089] >> val_loss: 35.3826\n",
      "Epoch [1090] >> val_loss: 37.1913\n",
      "Epoch [1091] >> val_loss: 39.0561\n",
      "Epoch [1092] >> val_loss: 40.1725\n",
      "Epoch [1093] >> val_loss: 35.8614\n",
      "Epoch [1094] >> val_loss: 37.3975\n",
      "Epoch [1095] >> val_loss: 36.6819\n",
      "Epoch [1096] >> val_loss: 36.4916\n",
      "Epoch [1097] >> val_loss: 35.1645\n",
      "Epoch [1098] >> val_loss: 36.2017\n",
      "Epoch [1099] >> val_loss: 37.2826\n",
      "Epoch [1100] >> val_loss: 36.9579\n",
      "Epoch [1101] >> val_loss: 37.2466\n",
      "Epoch [1102] >> val_loss: 36.6269\n",
      "Epoch [1103] >> val_loss: 36.4086\n",
      "Epoch [1104] >> val_loss: 35.8946\n",
      "Epoch [1105] >> val_loss: 36.2480\n",
      "Epoch [1106] >> val_loss: 35.5713\n",
      "Epoch [1107] >> val_loss: 39.2492\n",
      "Epoch [1108] >> val_loss: 37.7401\n",
      "Epoch [1109] >> val_loss: 35.4152\n",
      "Epoch [1110] >> val_loss: 34.8580\n",
      "Epoch [1111] >> val_loss: 39.0292\n",
      "Epoch [1112] >> val_loss: 39.7467\n",
      "Epoch [1113] >> val_loss: 34.8175\n",
      "Epoch [1114] >> val_loss: 37.4040\n",
      "Epoch [1115] >> val_loss: 36.5507\n",
      "Epoch [1116] >> val_loss: 37.9517\n",
      "Epoch [1117] >> val_loss: 35.7563\n",
      "Epoch [1118] >> val_loss: 38.9804\n",
      "Epoch [1119] >> val_loss: 38.0104\n",
      "Epoch [1120] >> val_loss: 38.7439\n",
      "Epoch [1121] >> val_loss: 38.9917\n",
      "Epoch [1122] >> val_loss: 34.8245\n",
      "Epoch [1123] >> val_loss: 37.2728\n",
      "Epoch [1124] >> val_loss: 41.8220\n",
      "Epoch [1125] >> val_loss: 35.0320\n",
      "Epoch [1126] >> val_loss: 35.9733\n",
      "Epoch [1127] >> val_loss: 39.4613\n",
      "Epoch [1128] >> val_loss: 36.6844\n",
      "Epoch [1129] >> val_loss: 36.0214\n",
      "Epoch [1130] >> val_loss: 35.4025\n",
      "Epoch [1131] >> val_loss: 37.6470\n",
      "Epoch [1132] >> val_loss: 41.1819\n",
      "Epoch [1133] >> val_loss: 38.9763\n",
      "Epoch [1134] >> val_loss: 38.6011\n",
      "Epoch [1135] >> val_loss: 37.2081\n",
      "Epoch [1136] >> val_loss: 36.8086\n",
      "Epoch [1137] >> val_loss: 36.7127\n",
      "Epoch [1138] >> val_loss: 37.5767\n",
      "Epoch [1139] >> val_loss: 39.8148\n",
      "Epoch [1140] >> val_loss: 37.9433\n",
      "Epoch [1141] >> val_loss: 36.8706\n",
      "Epoch [1142] >> val_loss: 38.2556\n",
      "Epoch [1143] >> val_loss: 35.8983\n",
      "Epoch [1144] >> val_loss: 36.5917\n",
      "Epoch [1145] >> val_loss: 35.6255\n",
      "Epoch [1146] >> val_loss: 36.0892\n",
      "Epoch [1147] >> val_loss: 37.4479\n",
      "Epoch [1148] >> val_loss: 36.8135\n",
      "Epoch [1149] >> val_loss: 36.0942\n",
      "Epoch [1150] >> val_loss: 40.5102\n",
      "Epoch [1151] >> val_loss: 37.5831\n",
      "Epoch [1152] >> val_loss: 35.8162\n",
      "Epoch [1153] >> val_loss: 36.8454\n",
      "Epoch [1154] >> val_loss: 34.6109\n",
      "Epoch [1155] >> val_loss: 36.7605\n",
      "Epoch [1156] >> val_loss: 37.6217\n",
      "Epoch [1157] >> val_loss: 38.7299\n",
      "Epoch [1158] >> val_loss: 35.3596\n",
      "Epoch [1159] >> val_loss: 34.9626\n",
      "Epoch [1160] >> val_loss: 38.8775\n",
      "Epoch [1161] >> val_loss: 34.2881\n",
      "Epoch [1162] >> val_loss: 36.3399\n",
      "Epoch [1163] >> val_loss: 36.3747\n",
      "Epoch [1164] >> val_loss: 38.6987\n",
      "Epoch [1165] >> val_loss: 37.0322\n",
      "Epoch [1166] >> val_loss: 38.9823\n",
      "Epoch [1167] >> val_loss: 36.0368\n",
      "Epoch [1168] >> val_loss: 38.6364\n",
      "Epoch [1169] >> val_loss: 38.3642\n",
      "Epoch [1170] >> val_loss: 36.2954\n",
      "Epoch [1171] >> val_loss: 38.3276\n",
      "Epoch [1172] >> val_loss: 36.1254\n",
      "Epoch [1173] >> val_loss: 43.2470\n",
      "Epoch [1174] >> val_loss: 37.0022\n",
      "Epoch [1175] >> val_loss: 39.6777\n",
      "Epoch [1176] >> val_loss: 36.2602\n",
      "Epoch [1177] >> val_loss: 38.9076\n",
      "Epoch [1178] >> val_loss: 40.1224\n",
      "Epoch [1179] >> val_loss: 39.1400\n",
      "Epoch [1180] >> val_loss: 38.9237\n",
      "Epoch [1181] >> val_loss: 38.1425\n",
      "Epoch [1182] >> val_loss: 41.9705\n",
      "Epoch [1183] >> val_loss: 45.1306\n",
      "Epoch [1184] >> val_loss: 40.7063\n",
      "Epoch [1185] >> val_loss: 38.1169\n",
      "Epoch [1186] >> val_loss: 35.4578\n",
      "Epoch [1187] >> val_loss: 35.8985\n",
      "Epoch [1188] >> val_loss: 37.7479\n",
      "Epoch [1189] >> val_loss: 37.5506\n",
      "Epoch [1190] >> val_loss: 37.6074\n",
      "Epoch [1191] >> val_loss: 36.9846\n",
      "Epoch [1192] >> val_loss: 38.1905\n",
      "Epoch [1193] >> val_loss: 38.0302\n",
      "Epoch [1194] >> val_loss: 38.3010\n",
      "Epoch [1195] >> val_loss: 36.2537\n",
      "Epoch [1196] >> val_loss: 34.2314\n",
      "Epoch [1197] >> val_loss: 37.0973\n",
      "Epoch [1198] >> val_loss: 35.0853\n",
      "Epoch [1199] >> val_loss: 37.0843\n",
      "Epoch [1200] >> val_loss: 37.2485\n",
      "Epoch [1201] >> val_loss: 38.3104\n",
      "Epoch [1202] >> val_loss: 36.9585\n",
      "Epoch [1203] >> val_loss: 35.0715\n",
      "Epoch [1204] >> val_loss: 34.7203\n",
      "Epoch [1205] >> val_loss: 39.5703\n",
      "Epoch [1206] >> val_loss: 38.9646\n",
      "Epoch [1207] >> val_loss: 36.2900\n",
      "Epoch [1208] >> val_loss: 39.3100\n",
      "Epoch [1209] >> val_loss: 38.3452\n",
      "Epoch [1210] >> val_loss: 36.8935\n",
      "Epoch [1211] >> val_loss: 35.9750\n",
      "Epoch [1212] >> val_loss: 37.9731\n",
      "Epoch [1213] >> val_loss: 36.2209\n",
      "Epoch [1214] >> val_loss: 37.2364\n",
      "Epoch [1215] >> val_loss: 35.1779\n",
      "Epoch [1216] >> val_loss: 34.7687\n",
      "Epoch [1217] >> val_loss: 37.2018\n",
      "Epoch [1218] >> val_loss: 39.1052\n",
      "Epoch [1219] >> val_loss: 38.4842\n",
      "Epoch [1220] >> val_loss: 39.2061\n",
      "Epoch [1221] >> val_loss: 36.0415\n",
      "Epoch [1222] >> val_loss: 39.3337\n",
      "Epoch [1223] >> val_loss: 36.9303\n",
      "Epoch [1224] >> val_loss: 34.9571\n",
      "Epoch [1225] >> val_loss: 38.4240\n",
      "Epoch [1226] >> val_loss: 37.2258\n",
      "Epoch [1227] >> val_loss: 35.7725\n",
      "Epoch [1228] >> val_loss: 36.7052\n",
      "Epoch [1229] >> val_loss: 38.4907\n",
      "Epoch [1230] >> val_loss: 40.3387\n",
      "Epoch [1231] >> val_loss: 36.7573\n",
      "Epoch [1232] >> val_loss: 35.3189\n",
      "Epoch [1233] >> val_loss: 35.9724\n",
      "Epoch [1234] >> val_loss: 35.3172\n",
      "Epoch [1235] >> val_loss: 34.8788\n",
      "Epoch [1236] >> val_loss: 36.6532\n",
      "Epoch [1237] >> val_loss: 39.6987\n",
      "Epoch [1238] >> val_loss: 37.7536\n",
      "Epoch [1239] >> val_loss: 40.1452\n",
      "Epoch [1240] >> val_loss: 37.2004\n",
      "Epoch [1241] >> val_loss: 38.3235\n",
      "Epoch [1242] >> val_loss: 35.4904\n",
      "Epoch [1243] >> val_loss: 37.7378\n",
      "Epoch [1244] >> val_loss: 36.3224\n",
      "Epoch [1245] >> val_loss: 38.0299\n",
      "Epoch [1246] >> val_loss: 39.7862\n",
      "Epoch [1247] >> val_loss: 36.9075\n",
      "Epoch [1248] >> val_loss: 35.6590\n",
      "Epoch [1249] >> val_loss: 38.4568\n",
      "Epoch [1250] >> val_loss: 35.0678\n",
      "Epoch [1251] >> val_loss: 34.6306\n",
      "Epoch [1252] >> val_loss: 36.2214\n",
      "Epoch [1253] >> val_loss: 35.8463\n",
      "Epoch [1254] >> val_loss: 36.8357\n",
      "Epoch [1255] >> val_loss: 35.7411\n",
      "Epoch [1256] >> val_loss: 37.0920\n",
      "Epoch [1257] >> val_loss: 36.1733\n",
      "Epoch [1258] >> val_loss: 37.0164\n",
      "Epoch [1259] >> val_loss: 36.5240\n",
      "Epoch [1260] >> val_loss: 36.8336\n",
      "Epoch [1261] >> val_loss: 38.3675\n",
      "Epoch [1262] >> val_loss: 39.5281\n",
      "Epoch [1263] >> val_loss: 37.7417\n",
      "Epoch [1264] >> val_loss: 38.1905\n",
      "Epoch [1265] >> val_loss: 40.4442\n",
      "Epoch [1266] >> val_loss: 35.5602\n",
      "Epoch [1267] >> val_loss: 41.8830\n",
      "Epoch [1268] >> val_loss: 41.0414\n",
      "Epoch [1269] >> val_loss: 37.2533\n",
      "Epoch [1270] >> val_loss: 34.9443\n",
      "Epoch [1271] >> val_loss: 43.6008\n",
      "Epoch [1272] >> val_loss: 42.0878\n",
      "Epoch [1273] >> val_loss: 37.2504\n",
      "Epoch [1274] >> val_loss: 38.2958\n",
      "Epoch [1275] >> val_loss: 35.9383\n",
      "Epoch [1276] >> val_loss: 36.8989\n",
      "Epoch [1277] >> val_loss: 36.1960\n",
      "Epoch [1278] >> val_loss: 36.2441\n",
      "Epoch [1279] >> val_loss: 37.4355\n",
      "Epoch [1280] >> val_loss: 38.8636\n",
      "Epoch [1281] >> val_loss: 38.6926\n",
      "Epoch [1282] >> val_loss: 37.1452\n",
      "Epoch [1283] >> val_loss: 35.4364\n",
      "Epoch [1284] >> val_loss: 37.2935\n",
      "Epoch [1285] >> val_loss: 36.6148\n",
      "Epoch [1286] >> val_loss: 36.2322\n",
      "Epoch [1287] >> val_loss: 34.7135\n",
      "Epoch [1288] >> val_loss: 36.0615\n",
      "Epoch [1289] >> val_loss: 35.2254\n",
      "Epoch [1290] >> val_loss: 39.7532\n",
      "Epoch [1291] >> val_loss: 37.3781\n",
      "Epoch [1292] >> val_loss: 39.1702\n",
      "Epoch [1293] >> val_loss: 35.0318\n",
      "Epoch [1294] >> val_loss: 37.2912\n",
      "Epoch [1295] >> val_loss: 41.0921\n",
      "Epoch [1296] >> val_loss: 37.4168\n",
      "Epoch [1297] >> val_loss: 35.0228\n",
      "Epoch [1298] >> val_loss: 40.9603\n",
      "Epoch [1299] >> val_loss: 35.6144\n",
      "Epoch [1300] >> val_loss: 36.9350\n",
      "Epoch [1301] >> val_loss: 37.3617\n",
      "Epoch [1302] >> val_loss: 37.0099\n",
      "Epoch [1303] >> val_loss: 38.8445\n",
      "Epoch [1304] >> val_loss: 37.3354\n",
      "Epoch [1305] >> val_loss: 35.8691\n",
      "Epoch [1306] >> val_loss: 41.7561\n",
      "Epoch [1307] >> val_loss: 39.9926\n",
      "Epoch [1308] >> val_loss: 36.5536\n",
      "Epoch [1309] >> val_loss: 35.3496\n",
      "Epoch [1310] >> val_loss: 36.7908\n",
      "Epoch [1311] >> val_loss: 38.1693\n",
      "Epoch [1312] >> val_loss: 41.1127\n",
      "Epoch [1313] >> val_loss: 35.0880\n",
      "Epoch [1314] >> val_loss: 37.3637\n",
      "Epoch [1315] >> val_loss: 36.2850\n",
      "Epoch [1316] >> val_loss: 37.8548\n",
      "Epoch [1317] >> val_loss: 36.3851\n",
      "Epoch [1318] >> val_loss: 37.8590\n",
      "Epoch [1319] >> val_loss: 37.9492\n",
      "Epoch [1320] >> val_loss: 35.0942\n",
      "Epoch [1321] >> val_loss: 40.4934\n",
      "Epoch [1322] >> val_loss: 35.6573\n",
      "Epoch [1323] >> val_loss: 39.9544\n",
      "Epoch [1324] >> val_loss: 36.9804\n",
      "Epoch [1325] >> val_loss: 38.5637\n",
      "Epoch [1326] >> val_loss: 35.9332\n",
      "Epoch [1327] >> val_loss: 40.6360\n",
      "Epoch [1328] >> val_loss: 35.1644\n",
      "Epoch [1329] >> val_loss: 35.3323\n",
      "Epoch [1330] >> val_loss: 36.8287\n",
      "Epoch [1331] >> val_loss: 35.9914\n",
      "Epoch [1332] >> val_loss: 36.0900\n",
      "Epoch [1333] >> val_loss: 35.1500\n",
      "Epoch [1334] >> val_loss: 37.4578\n",
      "Epoch [1335] >> val_loss: 33.6746\n",
      "Epoch [1336] >> val_loss: 35.0305\n",
      "Epoch [1337] >> val_loss: 37.4088\n",
      "Epoch [1338] >> val_loss: 36.8800\n",
      "Epoch [1339] >> val_loss: 38.3249\n",
      "Epoch [1340] >> val_loss: 45.7183\n",
      "Epoch [1341] >> val_loss: 34.9287\n",
      "Epoch [1342] >> val_loss: 36.8362\n",
      "Epoch [1343] >> val_loss: 37.4000\n",
      "Epoch [1344] >> val_loss: 35.0494\n",
      "Epoch [1345] >> val_loss: 37.2288\n",
      "Epoch [1346] >> val_loss: 40.8347\n",
      "Epoch [1347] >> val_loss: 39.6395\n",
      "Epoch [1348] >> val_loss: 36.4192\n",
      "Epoch [1349] >> val_loss: 37.0729\n",
      "Epoch [1350] >> val_loss: 35.7757\n",
      "Epoch [1351] >> val_loss: 42.7482\n",
      "Epoch [1352] >> val_loss: 35.1486\n",
      "Epoch [1353] >> val_loss: 35.6918\n",
      "Epoch [1354] >> val_loss: 36.0160\n",
      "Epoch [1355] >> val_loss: 36.1601\n",
      "Epoch [1356] >> val_loss: 35.8747\n",
      "Epoch [1357] >> val_loss: 37.3915\n",
      "Epoch [1358] >> val_loss: 35.0742\n",
      "Epoch [1359] >> val_loss: 37.4155\n",
      "Epoch [1360] >> val_loss: 35.3262\n",
      "Epoch [1361] >> val_loss: 35.6226\n",
      "Epoch [1362] >> val_loss: 40.2688\n",
      "Epoch [1363] >> val_loss: 36.9925\n",
      "Epoch [1364] >> val_loss: 41.2563\n",
      "Epoch [1365] >> val_loss: 42.7404\n",
      "Epoch [1366] >> val_loss: 35.8771\n",
      "Epoch [1367] >> val_loss: 37.0603\n",
      "Epoch [1368] >> val_loss: 36.4910\n",
      "Epoch [1369] >> val_loss: 35.3313\n",
      "Epoch [1370] >> val_loss: 35.2168\n",
      "Epoch [1371] >> val_loss: 38.5638\n",
      "Epoch [1372] >> val_loss: 36.4209\n",
      "Epoch [1373] >> val_loss: 34.2654\n",
      "Epoch [1374] >> val_loss: 35.8165\n",
      "Epoch [1375] >> val_loss: 35.2004\n",
      "Epoch [1376] >> val_loss: 36.8825\n",
      "Epoch [1377] >> val_loss: 38.3632\n",
      "Epoch [1378] >> val_loss: 37.0608\n",
      "Epoch [1379] >> val_loss: 38.4001\n",
      "Epoch [1380] >> val_loss: 36.3423\n",
      "Epoch [1381] >> val_loss: 37.8992\n",
      "Epoch [1382] >> val_loss: 37.0733\n",
      "Epoch [1383] >> val_loss: 34.1634\n",
      "Epoch [1384] >> val_loss: 35.4461\n",
      "Epoch [1385] >> val_loss: 36.0498\n",
      "Epoch [1386] >> val_loss: 36.6682\n",
      "Epoch [1387] >> val_loss: 35.4467\n",
      "Epoch [1388] >> val_loss: 34.5361\n",
      "Epoch [1389] >> val_loss: 39.8173\n",
      "Epoch [1390] >> val_loss: 37.2412\n",
      "Epoch [1391] >> val_loss: 37.0710\n",
      "Epoch [1392] >> val_loss: 39.8260\n",
      "Epoch [1393] >> val_loss: 34.5194\n",
      "Epoch [1394] >> val_loss: 40.9505\n",
      "Epoch [1395] >> val_loss: 38.1208\n",
      "Epoch [1396] >> val_loss: 37.2352\n",
      "Epoch [1397] >> val_loss: 39.4083\n",
      "Epoch [1398] >> val_loss: 34.9950\n",
      "Epoch [1399] >> val_loss: 37.5138\n",
      "Epoch [1400] >> val_loss: 35.3572\n",
      "Epoch [1401] >> val_loss: 36.0031\n",
      "Epoch [1402] >> val_loss: 35.9507\n",
      "Epoch [1403] >> val_loss: 36.9759\n",
      "Epoch [1404] >> val_loss: 34.6102\n",
      "Epoch [1405] >> val_loss: 40.7057\n",
      "Epoch [1406] >> val_loss: 36.9844\n",
      "Epoch [1407] >> val_loss: 35.5181\n",
      "Epoch [1408] >> val_loss: 37.5512\n",
      "Epoch [1409] >> val_loss: 36.0382\n",
      "Epoch [1410] >> val_loss: 38.7887\n",
      "Epoch [1411] >> val_loss: 33.8018\n",
      "Epoch [1412] >> val_loss: 35.6085\n",
      "Epoch [1413] >> val_loss: 38.9437\n",
      "Epoch [1414] >> val_loss: 37.7832\n",
      "Epoch [1415] >> val_loss: 36.2749\n",
      "Epoch [1416] >> val_loss: 34.3968\n",
      "Epoch [1417] >> val_loss: 35.8570\n",
      "Epoch [1418] >> val_loss: 36.6573\n",
      "Epoch [1419] >> val_loss: 37.7352\n",
      "Epoch [1420] >> val_loss: 34.8825\n",
      "Epoch [1421] >> val_loss: 34.8415\n",
      "Epoch [1422] >> val_loss: 38.1531\n",
      "Epoch [1423] >> val_loss: 35.2776\n",
      "Epoch [1424] >> val_loss: 35.7656\n",
      "Epoch [1425] >> val_loss: 37.0732\n",
      "Epoch [1426] >> val_loss: 39.1516\n",
      "Epoch [1427] >> val_loss: 37.8590\n",
      "Epoch [1428] >> val_loss: 36.0396\n",
      "Epoch [1429] >> val_loss: 35.2844\n",
      "Epoch [1430] >> val_loss: 38.7476\n",
      "Epoch [1431] >> val_loss: 35.4563\n",
      "Epoch [1432] >> val_loss: 34.2462\n",
      "Epoch [1433] >> val_loss: 37.9751\n",
      "Epoch [1434] >> val_loss: 40.4724\n",
      "Epoch [1435] >> val_loss: 41.4577\n",
      "Epoch [1436] >> val_loss: 35.5662\n",
      "Epoch [1437] >> val_loss: 38.5977\n",
      "Epoch [1438] >> val_loss: 34.1088\n",
      "Epoch [1439] >> val_loss: 35.9799\n",
      "Epoch [1440] >> val_loss: 38.4139\n",
      "Epoch [1441] >> val_loss: 37.8872\n",
      "Epoch [1442] >> val_loss: 35.9543\n",
      "Epoch [1443] >> val_loss: 35.0932\n",
      "Epoch [1444] >> val_loss: 37.6802\n",
      "Epoch [1445] >> val_loss: 37.9044\n",
      "Epoch [1446] >> val_loss: 37.9821\n",
      "Epoch [1447] >> val_loss: 35.6362\n",
      "Epoch [1448] >> val_loss: 35.0073\n",
      "Epoch [1449] >> val_loss: 37.5904\n",
      "Epoch [1450] >> val_loss: 37.1973\n",
      "Epoch [1451] >> val_loss: 34.9105\n",
      "Epoch [1452] >> val_loss: 35.5367\n",
      "Epoch [1453] >> val_loss: 34.7041\n",
      "Epoch [1454] >> val_loss: 36.2997\n",
      "Epoch [1455] >> val_loss: 35.2569\n",
      "Epoch [1456] >> val_loss: 37.6346\n",
      "Epoch [1457] >> val_loss: 37.7379\n",
      "Epoch [1458] >> val_loss: 36.0185\n",
      "Epoch [1459] >> val_loss: 40.1979\n",
      "Epoch [1460] >> val_loss: 34.6116\n",
      "Epoch [1461] >> val_loss: 35.3878\n",
      "Epoch [1462] >> val_loss: 35.4701\n",
      "Epoch [1463] >> val_loss: 38.1838\n",
      "Epoch [1464] >> val_loss: 36.7267\n",
      "Epoch [1465] >> val_loss: 39.6607\n",
      "Epoch [1466] >> val_loss: 36.9798\n",
      "Epoch [1467] >> val_loss: 35.7273\n",
      "Epoch [1468] >> val_loss: 34.6581\n",
      "Epoch [1469] >> val_loss: 35.2313\n",
      "Epoch [1470] >> val_loss: 34.5734\n",
      "Epoch [1471] >> val_loss: 40.7001\n",
      "Epoch [1472] >> val_loss: 35.3727\n",
      "Epoch [1473] >> val_loss: 37.0195\n",
      "Epoch [1474] >> val_loss: 39.3092\n",
      "Epoch [1475] >> val_loss: 37.7608\n",
      "Epoch [1476] >> val_loss: 38.6336\n",
      "Epoch [1477] >> val_loss: 39.8843\n",
      "Epoch [1478] >> val_loss: 37.4055\n",
      "Epoch [1479] >> val_loss: 37.7900\n",
      "Epoch [1480] >> val_loss: 37.1019\n",
      "Epoch [1481] >> val_loss: 35.0887\n",
      "Epoch [1482] >> val_loss: 35.5661\n",
      "Epoch [1483] >> val_loss: 39.1679\n",
      "Epoch [1484] >> val_loss: 40.6857\n",
      "Epoch [1485] >> val_loss: 36.5122\n",
      "Epoch [1486] >> val_loss: 36.5519\n",
      "Epoch [1487] >> val_loss: 36.7028\n",
      "Epoch [1488] >> val_loss: 33.5118\n",
      "Epoch [1489] >> val_loss: 38.4061\n",
      "Epoch [1490] >> val_loss: 37.3344\n",
      "Epoch [1491] >> val_loss: 37.7608\n",
      "Epoch [1492] >> val_loss: 35.0027\n",
      "Epoch [1493] >> val_loss: 39.2047\n",
      "Epoch [1494] >> val_loss: 41.8180\n",
      "Epoch [1495] >> val_loss: 40.9358\n",
      "Epoch [1496] >> val_loss: 36.3635\n",
      "Epoch [1497] >> val_loss: 36.5372\n",
      "Epoch [1498] >> val_loss: 35.3313\n",
      "Epoch [1499] >> val_loss: 36.2568\n",
      "Epoch [1500] >> val_loss: 34.9837\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "lr = 1e-3\n",
    "history = fit(epochs, lr, model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('linear1.weight',\n              tensor([[ 3.2895e-01, -1.6357e+00, -2.3684e+00,  7.8153e-02,  4.8248e+00,\n                        1.3472e-01,  9.6757e-01,  3.7392e-01, -8.3948e+00, -6.6449e+00,\n                        5.8282e-01,  2.5847e+00],\n                      [-6.0197e-01,  5.2537e-01, -3.3140e-04,  2.8365e+00,  3.2720e-01,\n                       -3.2082e+00,  1.7723e+00,  2.6871e-01, -2.6532e-01, -4.9566e-01,\n                        6.9863e-01, -2.6898e-01],\n                      [-4.0512e-01,  3.4261e+01,  5.2678e-02, -2.5116e+00,  3.0468e+00,\n                       -1.0589e-01,  1.4745e+00, -1.1441e-01,  1.4619e-01,  1.0690e+00,\n                       -4.9231e-01,  2.1755e-01],\n                      [ 6.8193e-02,  3.6987e-01, -2.2754e-02,  4.8296e+00, -1.3057e-01,\n                       -8.5392e-02, -2.4458e+01, -8.9532e-02,  2.2887e+00, -5.9552e-01,\n                       -3.7468e-01, -3.2393e-01],\n                      [-9.4536e-02, -2.6538e+00, -4.1978e-02,  1.6437e+00,  1.8738e+00,\n                       -3.9439e-02,  4.0747e+00,  3.3102e-01, -1.0981e+00,  2.7703e-01,\n                       -6.8890e-01,  1.8929e-01],\n                      [ 1.8371e-01, -2.7781e+00, -2.2529e-01,  2.3515e+00,  2.2454e+00,\n                        3.5260e-01,  2.5132e+00, -1.7690e+00, -2.2170e+00, -1.7431e+00,\n                       -2.3209e+00, -1.0643e+00],\n                      [-7.7337e-01, -1.3702e+00,  6.4544e-01,  2.2305e+00,  2.4453e-01,\n                       -2.4554e+00, -1.5027e+00,  8.2818e-01, -8.4195e-01, -7.6771e-01,\n                        3.6837e-01, -2.6752e-01],\n                      [-2.6004e-01, -1.0588e+00, -3.0651e-02,  1.6772e+00, -4.3851e+00,\n                        2.9562e-01, -2.3448e+00,  8.1454e-01, -7.8946e+00, -9.4271e+00,\n                        6.4081e+00, -1.6314e+00],\n                      [-4.4807e-01, -3.6299e-01, -6.1688e-03,  2.6869e+00, -2.1057e-01,\n                       -2.4899e+00, -1.7284e+00, -4.8291e-02, -1.0898e+00, -6.6093e-01,\n                       -8.2425e-01,  9.4801e-02],\n                      [-7.6011e-02, -7.6206e-01,  5.8824e-02,  2.3910e+00, -2.4897e-01,\n                       -5.6044e-01, -6.3983e+00,  1.9385e+00, -1.8208e-01,  4.7754e-01,\n                        1.4419e+00,  1.4373e-02],\n                      [-9.6323e-01, -6.7915e-02, -2.1884e+00,  2.2847e+00, -1.5397e-01,\n                        7.7674e-01, -1.0790e+01,  1.6212e+00, -6.0359e+00, -6.1711e+00,\n                        2.3694e+00,  5.4034e-01],\n                      [ 2.2509e-01, -6.1602e-01, -1.1404e-01,  1.6342e+00,  9.1333e-01,\n                       -2.7707e-01,  3.6561e+00,  1.6809e-01, -9.0203e-01, -2.3431e-01,\n                        4.4315e-01,  2.0244e-01],\n                      [-2.5000e-01, -1.0614e+00, -6.0169e-01,  3.0060e+00,  4.7594e-02,\n                       -1.0796e+00,  7.4104e-01, -1.2891e+00, -1.2864e+00, -9.7271e-01,\n                       -1.8934e+00, -7.4098e-01],\n                      [-3.2524e-01, -1.0683e+00,  1.8405e-01,  2.7656e+00, -3.2893e+00,\n                        7.4296e-01, -1.7658e+01, -1.0015e-01, -2.6157e+00, -2.7220e+00,\n                       -9.7429e-01, -2.1389e+00],\n                      [-5.4143e-01,  3.2071e+00,  1.4830e-01, -1.6307e+00,  1.7023e+00,\n                        4.1106e-01,  6.8285e-01, -6.8463e-01,  7.7611e-01,  1.9761e+00,\n                        1.4547e+00,  9.7045e-01],\n                      [-8.8192e-01,  1.4264e+00,  6.3207e-01,  1.8048e+00,  6.9643e-01,\n                        8.7818e-02, -7.0356e-02, -2.9255e+00,  6.3251e-05,  5.9665e-01,\n                        1.6606e-02, -2.0640e-01],\n                      [-9.7294e-01, -1.8992e+00,  4.1208e-01,  3.4061e+00,  8.2693e-01,\n                        1.3657e-01, -2.6551e+00, -8.8850e-01, -1.3307e+00,  1.1523e-01,\n                       -1.8530e+00, -8.6946e-01],\n                      [ 7.7837e-01,  9.9652e-01, -3.2503e-01,  1.9045e+00,  6.5556e-01,\n                        5.4812e-01,  9.8502e-01, -1.0364e+00, -5.3499e-01, -1.2357e-01,\n                       -1.6813e+00, -1.5883e+00],\n                      [-1.6981e+00,  1.9139e-01,  1.0692e+00,  9.7168e-01, -4.8513e-01,\n                        6.4864e-01, -4.4064e+00, -3.0402e-01, -8.6285e-01, -5.6518e-01,\n                       -5.7256e+00, -1.5032e+00],\n                      [-4.3861e-01,  5.4815e-01,  2.6947e-01,  1.7288e+00,  1.6507e+00,\n                        3.7928e-01,  8.1347e-01,  2.6234e+00,  6.5531e-01,  1.2925e+00,\n                        2.4352e+00,  2.1154e+00],\n                      [ 1.9158e+00,  2.3825e-01, -1.2113e+00, -2.7642e-01, -2.1604e+00,\n                       -1.8231e-02,  2.8065e+00, -2.2875e+00,  1.3529e+01,  1.2668e+01,\n                       -4.4999e+00, -5.3692e+00],\n                      [-1.4513e-01, -8.9154e-01, -6.4765e-02,  3.6060e+00,  2.6208e+00,\n                       -4.6022e-01, -2.3917e+01, -3.9219e-01, -3.3137e-02, -3.7551e-01,\n                       -5.2644e-01, -1.2634e+00],\n                      [-2.6778e-01, -6.1019e+00,  3.3196e-01,  1.9010e-01, -8.7841e-01,\n                        4.8400e-01,  7.3883e+00, -1.1792e+00, -3.6304e-01,  1.2767e+00,\n                       -2.2414e+00, -3.4047e-01],\n                      [-8.0205e-01, -1.6917e+00, -1.6245e+00,  2.0921e+00, -2.2132e+00,\n                        5.5651e-01, -1.3447e+00,  5.4777e-01, -9.0298e+00, -7.2737e+00,\n                        1.6446e+00, -2.2117e+00]], device='cuda:0')),\n             ('linear1.bias',\n              tensor([ 12.1448,  -2.1811,  13.0361, -18.9729,   1.5281,  -8.1751,  -4.1226,\n                      -22.8936,  -3.2272, -11.0185,  -7.1392,  -3.5417,  -5.1131, -23.2751,\n                       29.6954,  -9.8914, -17.1171, -10.6382,  -7.5586, -20.2651,   1.1448,\n                      -31.5260,  -0.4217,  -5.5388], device='cuda:0')),\n             ('linear2.weight',\n              tensor([[ 5.9039e-01,  2.8957e-01, -3.9554e-01, -3.6806e-01, -3.1288e-01,\n                        4.5329e-01,  5.5341e-01, -9.1388e-01,  2.8872e-01, -5.3869e-01,\n                       -9.5752e-02,  8.2765e-01,  8.6607e-02, -8.6852e-01, -3.1327e-01,\n                       -3.3260e-01,  1.0016e+00, -9.7736e-01, -3.8052e-01,  5.3072e-01,\n                        2.7284e-01, -1.2471e+00,  4.5037e-01,  1.6678e+00],\n                      [-3.5325e+00,  2.3960e+00,  3.9137e+00, -9.6194e+00, -5.6277e+00,\n                        2.6365e+00,  8.2829e-01, -2.8788e+00, -1.9452e-02,  2.0600e+00,\n                        4.7709e-01, -8.3074e+00,  1.3705e+00,  1.0141e-01,  6.0507e+00,\n                        3.3945e+00,  2.7543e+00,  3.7149e+00,  1.8525e+00,  4.4762e+00,\n                        1.4591e+00,  2.7688e+00,  1.2199e+00,  9.3994e-01],\n                      [-3.4899e+00, -4.3450e-01,  8.1193e+00, -1.4256e+01, -9.3759e+00,\n                        3.3695e+00,  2.0597e+00, -4.0465e+00,  1.5394e+00,  4.0911e+00,\n                       -4.0457e+00, -7.6960e+00,  1.9201e+00,  5.8658e+00,  5.3439e-01,\n                        1.8265e+00,  3.8000e+00,  2.5614e+00,  1.2797e+00,  4.3112e+00,\n                        3.4015e+00,  5.9934e+00,  4.5707e+00,  3.7989e+00],\n                      [-1.4296e+00,  5.1780e-01,  3.5586e-02, -2.7973e+00, -8.3424e-01,\n                        1.1114e+00,  2.7525e-02,  7.6108e-01,  1.1529e+00,  1.2167e+00,\n                       -9.0937e-01, -5.0707e+00,  4.8976e-01,  5.4260e-01,  3.8072e+00,\n                        1.1877e+00,  5.9432e-01,  3.4707e+00, -7.1814e-01,  2.0705e+00,\n                        5.9542e-01,  1.4358e+00, -1.5958e-01, -4.3347e-01],\n                      [ 2.6680e-01, -2.2724e-01, -2.8109e-02,  8.3570e-02, -3.7348e-02,\n                        1.0835e-01, -2.0454e-01, -7.6800e-01,  7.4592e-02,  1.1920e-01,\n                       -4.8099e-02, -1.3914e-01,  6.8285e-02,  1.1072e+00,  5.0241e-01,\n                       -5.1619e-02,  4.8558e-02, -1.5320e-02,  2.6222e-01,  4.2018e-01,\n                       -1.1234e+00,  1.2267e+00, -6.5932e-01, -1.1030e+00],\n                      [-6.1686e-02,  1.4911e-01,  1.6771e-01, -4.8573e-02, -1.2517e-01,\n                       -4.5461e-02, -1.0017e-01,  5.8083e-02, -1.1094e-01,  9.8162e-02,\n                       -2.9123e-02,  7.4148e-02,  1.2876e-01,  2.9747e-01, -1.7504e-01,\n                       -1.3900e-02,  6.8586e-02,  1.1658e-01,  2.6818e-02,  8.8078e-02,\n                       -2.3499e-01,  2.9928e-01, -1.4724e-01, -3.0791e-01],\n                      [ 3.4646e-01,  1.1565e-01, -2.6757e-01, -1.2894e-01, -3.6915e-02,\n                        7.0743e-02, -7.7694e-02, -1.7175e-01, -8.0512e-02,  1.2468e-01,\n                       -4.8777e-02, -1.7909e-01, -5.5176e-02, -8.9296e-02,  1.9780e-01,\n                        1.8050e-01, -1.8121e-02,  1.2391e-01,  5.1373e-03,  3.8024e-02,\n                        6.4909e-02,  1.0619e-01, -2.5355e-01,  1.9883e-01],\n                      [-1.8768e-01, -1.0603e-01, -3.6839e-02, -1.4245e-01, -1.7056e-01,\n                       -1.8353e-01,  2.7234e-03,  9.4676e-02, -1.2595e-01, -1.6739e-01,\n                       -1.1812e-01,  2.8097e-02,  1.6235e-02, -6.3581e-02, -3.0869e-02,\n                        1.2812e-01,  1.9622e-01,  2.0832e-02,  7.4286e-02,  1.8360e-01,\n                       -2.0269e-01, -1.6182e-01, -1.1362e-01,  1.7842e-01],\n                      [-3.5820e+00,  6.2461e-01,  1.1209e+00, -5.6319e+00, -3.5621e+00,\n                        1.8136e+00,  1.2497e+00,  8.8083e-01,  1.0319e+00,  5.7531e-01,\n                       -2.0966e+00, -3.4975e+00,  5.0980e-01,  2.4299e+00,  1.2421e+00,\n                        2.2278e+00,  1.5241e+00,  5.1524e-01,  3.3149e+00,  2.5744e+00,\n                        2.4944e+00,  4.9938e+00,  2.6315e-01, -1.5487e-01],\n                      [-4.2105e-02, -3.7269e-02,  1.3990e-01, -8.9835e-02,  8.7758e-04,\n                        7.5061e-02, -4.2837e-03, -1.4888e-01, -1.8835e-01,  1.3695e-01,\n                        1.3337e-01, -1.0645e-01, -7.4722e-02, -1.9768e-01, -3.0777e-02,\n                       -1.5466e-01,  5.1955e-03,  1.8983e-01,  3.8303e-02, -1.0667e-01,\n                        1.8031e-01, -1.1099e-01, -1.2037e-01, -8.8440e-02],\n                      [-9.3560e-02, -1.2869e-01, -1.1311e+00,  6.0675e-01,  9.8968e-02,\n                        7.0949e-02,  1.9365e-01, -3.6529e-01,  2.5585e-01,  1.6635e-01,\n                        3.4249e-01,  6.7635e-02,  2.6485e-01,  9.4912e-01, -9.1348e-01,\n                        3.3676e-02,  3.7257e-01,  1.7172e-01,  5.9179e-01, -1.7890e-01,\n                       -1.6161e+00,  1.3224e+00, -6.6529e-01, -6.1271e-01],\n                      [-7.9950e-02,  3.7745e-02, -9.8723e-01,  9.7535e-01, -1.6178e-01,\n                       -2.2195e-02,  1.8107e-01, -1.2883e-01,  1.8234e-01,  1.7245e-01,\n                        1.6096e-01, -7.8590e-02, -1.8055e-01,  5.8028e-01, -7.5760e-01,\n                        5.0727e-01,  2.2717e-01,  1.1226e-01,  3.8029e-01,  8.0034e-02,\n                       -9.1358e-01,  7.2330e-01, -1.6913e-01, -8.2386e-01]], device='cuda:0')),\n             ('linear2.bias',\n              tensor([ -0.6936,  -4.9955, -10.6061,   1.1404,  -0.3720,  -0.1737,  -0.3107,\n                       -0.1039,  -4.3250,  -0.1873,   0.0821,  -0.0767], device='cuda:0')),\n             ('linear3.weight',\n              tensor([[-3.2457, 15.5911, 24.4379,  7.7198, -2.5783, -0.5342, -0.4924,  0.1832,\n                       11.2427,  0.0886, -2.9302, -2.1581]], device='cuda:0')),\n             ('linear3.bias', tensor([4.2076], device='cuda:0'))])"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "state_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "torch.save(state_dict, 'sol_model.tar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "df = pd.read_csv('evaluation_data.csv')\n",
    "df1 = df.drop(['dteday'], axis=1)\n",
    "\n",
    "test_tensor = torch.from_numpy(df1.values.astype(float)).to(device).float()\n",
    "final_pred = model(test_tensor)\n",
    "predictions = final_pred.cpu().detach().numpy()\n",
    "pd.DataFrame(predictions).to_csv(\"evaluation_data.csv\",header=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}